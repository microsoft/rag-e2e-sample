{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "\n",
    "# specify the name of the .env file name \n",
    "env_name = \"llm_pgvector.env\" # change to your own .env file name\n",
    "config = dotenv_values(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract data and context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define filter ids here\n",
    "filter_id1_name = \"\"  \n",
    "filter_id2_name = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to extract text from pdf \n",
    "\"\"\"\n",
    "This code sample shows Prebuilt Document operations with the Azure Form Recognizer client library. \n",
    "The async versions of the samples require Python 3.6 or later.\n",
    "\n",
    "To learn more, please visit the documentation - Quickstart: Form Recognizer Python client library SDKs\n",
    "https://docs.microsoft.com/en-us/azure/applied-ai-services/form-recognizer/quickstarts/try-v3-python-sdk\n",
    "\"\"\"\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "\n",
    "\"\"\"\n",
    "Remember to remove the key from your code when you're done, and never post it publicly. For production, use\n",
    "secure methods to store and access your credentials. For more information, see \n",
    "https://docs.microsoft.com/en-us/azure/cognitive-services/cognitive-services-security?tabs=command-line%2Ccsharp#environment-variables-and-application-configuration\n",
    "\"\"\"\n",
    "\n",
    "endpoint = config[\"AZURE_FORM_RECOGNIZER_ENDPOINT\"]\n",
    "key = config[\"AZURE_FORM_RECOGNIZER_KEY\"]\n",
    "\n",
    "# sample document\n",
    "document_analysis_client = DocumentAnalysisClient(\n",
    "        endpoint=endpoint, credential=AzureKeyCredential(key)\n",
    "    )\n",
    "\n",
    "\n",
    "def analyze_pdf(doc_path):  \n",
    "    with open(doc_path, \"rb\") as f:\n",
    "        poller = document_analysis_client.begin_analyze_document(\n",
    "            \"prebuilt-document\", document=f\n",
    "        )\n",
    "    result = poller.result()\n",
    "    for kv_pair in result.key_value_pairs:\n",
    "\n",
    "        if filter_id1_name in kv_pair.key.content:\n",
    "            print(30*\"*\")\n",
    "            print(\"filter_id1_val:\")\n",
    "            print(\":\\n\")\n",
    "            print(kv_pair.value.content)\n",
    "            print(30*\"*\")\n",
    "                \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_values(text):\n",
    "    '''\n",
    "    given a text string, extract the client code and line of business\n",
    "    '''\n",
    "    filter_id1_pattern = r\"{filter_id1_name}\\s+([\\w,]+)\"\n",
    "    filter_id2_pattern = r\"{filter_id2_name}\\s+([\\w\\s]+)\"\n",
    "\n",
    "    filter_id1_match = re.search(filter_id1_pattern, text)\n",
    "    filter_id2_match = re.search(filter_id2_pattern, text)\n",
    "\n",
    "    if filter_id1_match and filter_id2_match:\n",
    "        filter_id1 = filter_id1_match.group(1).split(',')  # Split by commas to get a list\n",
    "        filter_id2 = filter_id2_match.group(1).split()[0]  # Extract only the first word\n",
    "        return filter_id1, filter_id2\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_line_page_tuples(result):\n",
    "    '''\n",
    "    Input: result of form recognizer analyze_pdf function\n",
    "    Output: Create list of tuples of the form (line, page_num, line_num) \n",
    "    This will keep reference of the line number and page number of each line in the document.\n",
    "    '''\n",
    "    line_page_tuples = []\n",
    "\n",
    "    total_pages = len(result.pages)\n",
    "    for page_num in range(total_pages):\n",
    "        lines = result.pages[page_num].lines\n",
    "        total_lines = len(lines)\n",
    "\n",
    "        for line_num in range(total_lines):\n",
    "            line = lines[line_num].content\n",
    "            line_page_tuples.append((line, page_num + 1, line_num + 1))\n",
    "\n",
    "    return line_page_tuples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_with_page_number(line_page_tuples, chunk_length = 10, chunk_overlap = 2):\n",
    "    '''\n",
    "    Given the list of tuples of the form (line, page_num, line_num) and chunk length and overlap,\n",
    "    it will create chunks of text with page number and line number of the first line in the chunk.\n",
    "    chunk length: number of lines in each chunk\n",
    "    chunk_overlap: number of overlapping lines between chunks\n",
    "    '''\n",
    "    pointer = 0 \n",
    "    chunks = []\n",
    "    total_lines = len(line_page_tuples)\n",
    "    #for line, page_number, line_number in line_page_tuples:\n",
    "    while pointer<total_lines:\n",
    "        line_count =0\n",
    "        current_chunk = \"\"\n",
    "        if not chunks: \n",
    "            # for first chunk we can not use overlap\n",
    "            pointer = 0\n",
    "        else:\n",
    "            pointer = pointer - chunk_overlap\n",
    "        \n",
    "        # take starting page number and line number \n",
    "        page_number, line_number = line_page_tuples[pointer][1:]  \n",
    "        while line_count<chunk_length and pointer<total_lines:\n",
    "            current_chunk = current_chunk + line_page_tuples[pointer][0]\n",
    "            current_chunk = current_chunk + \" \"\n",
    "            line_count += 1\n",
    "            pointer += 1\n",
    "        chunks.append((current_chunk, page_number, line_number))\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read pdf and extract the filter_entity2 and the line of business and assign DocId\n",
    "\n",
    "The DocId will be the same for the filter_entity2 and the line of business in a document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "doc_dir = \"..\\DATA\"\n",
    "pdf_files = [filename for filename in os.listdir(doc_dir) if filename.endswith('.pdf')]\n",
    "DocId = 0 \n",
    "for file in pdf_files:\n",
    "    DocId += 1\n",
    "    file_path = os.path.join(doc_dir, f\"{os.path.splitext(file)[0]}.pdf\")\n",
    "    # analyze the pdf using form recognizer\n",
    "    result = analyze_pdf(file_path)\n",
    "    # get the chunks in a tuple of the form (chunk, page_number, line_number)\n",
    "    line_page_tuples = create_line_page_tuples(result)\n",
    "    chunks = chunk_with_page_number(line_page_tuples = line_page_tuples, chunk_length = 10, chunk_overlap = 2)\n",
    "    # use the content to get the client code and line of business\n",
    "    values = extract_values(result.content)\n",
    "\n",
    "    if values:\n",
    "        filter_id1, filter_id2 = values\n",
    "        df = pd.DataFrame({\n",
    "            filter_id1_name : filter_id1,\n",
    "            filter_id2_name: filter_id2,\n",
    "            \"DocId\": DocId,\n",
    "        })\n",
    "    else:\n",
    "        print(\"Values not found.\")\n",
    "        df = pd.DataFrame({\n",
    "            filter_id1_name : \"[NULL]\",\n",
    "            filter_id2_name : \"NULL\",\n",
    "            \"DocId\": DocId,\n",
    "        })  \n",
    "        \n",
    "    print('writing the results of: \\n' + file)   \n",
    "    if not os.path.exists(\"../AnalyzedPDF/\"):\n",
    "        os.makedirs(\"../AnalyzedPDF/\") \n",
    "    if not os.path.exists(f\"../AnalyzedPDF/{filter_id1_name}/\"):\n",
    "        os.makedirs(f\"../AnalyzedPDF/{filter_id1_name}\")\n",
    "    if not os.path.exists(f\"../AnalyzedPDF/{filter_id1_name}\" + file +\".csv\"):\n",
    "        df.to_csv(f\"../AnalyzedPDF/{filter_id1_name}/\" + file +\".csv\" , index=False) \n",
    "    else:\n",
    "        print(f'File: {file}.csv already exists, skipping...')\n",
    "    \n",
    "    # write the chunks to another csv file \n",
    "    df_chunks = pd.DataFrame(chunks, columns = ['chunk', 'page_number', 'line_number'])  \n",
    "    df_chunks['DocId'] = DocId\n",
    "    if not os.path.exists(\"../AnalyzedPDF/Chunks/\"):\n",
    "        os.makedirs(\"../AnalyzedPDF/Chunks\")\n",
    "    df_chunks.to_csv(\"../AnalyzedPDF/Chunks/\" + 'DocId_'+ str(DocId) + \".csv\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv files in anlyzedPDF2 folder and create a dataframe\n",
    "import pandas as pd\n",
    "import os \n",
    "def read_csv_files(path= \"../AnalyzedPDF\"):\n",
    "    df = pd.DataFrame()\n",
    "    for file in os.listdir(path):\n",
    "        file_path = os.path.join(path, file)\n",
    "        df = pd.concat(pd.read_csv(file_path), ignore_index=True, axis = 0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd \n",
    "def concatenate_csv_files(path = \"../AnalyzedPDF\"):\n",
    "    csv_files = [file for file in os.listdir(path) if file.endswith(\".csv\")]\n",
    "    dfs = []\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(path, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        dfs.append(df)\n",
    "    print(\"Concatenated files:\", dfs)\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine all the csv files for filter_id and chunks and store in separate combined csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = f\"../AnalyzedPDF/{filter_id1_name}\"\n",
    "result_filter_id1_df = concatenate_csv_files(folder_path)\n",
    "print(result_filter_id1_df)\n",
    "if not os.path.exists(\"../AnalyzedPDF/CombinedResults\"):\n",
    "    os.makedirs(\"../AnalyzedPDF/CombinedResults\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's rename columns, add index, and save the results\n",
    "result_filter_id1_df[\"Id\"] = result_filter_id1_df.index +1 \n",
    "\n",
    "# now let's add a unique id\n",
    "columns = ['Id'] + [col for col in result_filter_id1_df.columns if col != 'Id']\n",
    "result_filter_id1_df = result_filter_id1_df.reindex(columns=columns)\n",
    "# rename for consistency\n",
    "result_filter_id1_df.columns = ['Id', 'DocId', filter_id1_name, filter_id2_name]\n",
    "result_filter_id1_df.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_filter_id1_df.to_csv(f\"../AnalyzedPDF/CombinedResults/{filter_id1_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"../AnalyzedPDF/Chunks\"\n",
    "result_chunk_df = concatenate_csv_files(folder_path)\n",
    "print(result_chunk_df)\n",
    "# add primary key\n",
    "result_chunk_df[\"Id\"] = result_chunk_df.index +1    \n",
    "#make Id as the first column as it will be used as primary key\n",
    "columns = ['Id'] + [col for col in result_chunk_df.columns if col != 'Id']\n",
    "result_chunk_df = result_chunk_df.reindex(columns=columns)\n",
    "new_columns = {\n",
    "    'Id': \"Id\",\n",
    "    'chunk': 'Chunk',\n",
    "    'Embedding': 'Embedding',\n",
    "    'page_number': 'PageNumber',\n",
    "    'DocID': 'DocID',\n",
    "    'line_number': 'LineNumber',\n",
    "}\n",
    "result_chunk_df = result_chunk_df.rename(columns=new_columns)\n",
    "# Print the DataFrame with 'Id' as the first column after index\n",
    "result_chunk_df.head(1000)\n",
    "result_chunk_df.to_csv(\"../AnalyzedPDF/CombinedResults/Chunks.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_chunk_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_filter_id1_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
