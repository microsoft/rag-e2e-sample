{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "\n",
    "# specify the name of the .env file name \n",
    "env_name = \"llm_pgvector.env\" # change to your own .env file name\n",
    "config = dotenv_values(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract data and context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to extract text from pdf \n",
    "\"\"\"\n",
    "This code sample shows Prebuilt Document operations with the Azure Form Recognizer client library. \n",
    "The async versions of the samples require Python 3.6 or later.\n",
    "\n",
    "To learn more, please visit the documentation - Quickstart: Form Recognizer Python client library SDKs\n",
    "https://docs.microsoft.com/en-us/azure/applied-ai-services/form-recognizer/quickstarts/try-v3-python-sdk\n",
    "\"\"\"\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "\n",
    "\"\"\"\n",
    "Remember to remove the key from your code when you're done, and never post it publicly. For production, use\n",
    "secure methods to store and access your credentials. For more information, see \n",
    "https://docs.microsoft.com/en-us/azure/cognitive-services/cognitive-services-security?tabs=command-line%2Ccsharp#environment-variables-and-application-configuration\n",
    "\"\"\"\n",
    "\n",
    "endpoint = config[\"AZURE_FORM_RECOGNIZER_ENDPOINT\"]\n",
    "key = config[\"AZURE_FORM_RECOGNIZER_KEY\"]\n",
    "\n",
    "# sample document\n",
    "document_analysis_client = DocumentAnalysisClient(\n",
    "        endpoint=endpoint, credential=AzureKeyCredential(key)\n",
    "    )\n",
    "\n",
    "\n",
    "def analyze_pdf(doc_path):  \n",
    "    with open(doc_path, \"rb\") as f:\n",
    "        poller = document_analysis_client.begin_analyze_document(\n",
    "            \"prebuilt-document\", document=f\n",
    "        )\n",
    "    result = poller.result()\n",
    "                \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Extract value if explicity mentioned in a text. You may develop the template for specific use case\n",
    "extract_name1 = \"Ticker\"  \n",
    "extract_name2= \"Quarter\"\n",
    "def extract_values(text, target =[\"FY\"]):\n",
    "    '''\n",
    "    given a text string, extract the client code and line of business\n",
    "    '''\n",
    "    extract_name1_pattern = r\"{extract_name1}\\s+([\\w,]+)\"\n",
    "    extract_name2_pattern = r\"{extract_name2}\\s+([\\w\\s]+)\"\n",
    "\n",
    "    extract_name1_match = re.search(extract_name1_pattern, text)\n",
    "    extract_name1_match = re.search(extract_name2_pattern, text)\n",
    "\n",
    "    if extract_name1_match and extract_name1_match:\n",
    "        extract_name1 = filter_id1_match.group(1).split(',')  # Split by commas to get a list\n",
    "        extract_name2 = filter_id2_match.group(1).split()[0]  # Extract only the first word\n",
    "        return extract_name1, extract_name2\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# extract information form the filename\n",
    "def extract_info_from_filename(filename):\n",
    "    pattern = r'([A-Z]+)TranscriptFY(\\d{2})Q(\\d)'\n",
    "    match = re.search(pattern, filename)\n",
    "    \n",
    "    if match:\n",
    "        symbol = match.group(1)\n",
    "        fiscal_year = match.group(2)\n",
    "        fiscal_quarter = match.group(3)\n",
    "        return symbol, fiscal_year, fiscal_quarter\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# example\n",
    "filename = \"MSFTTranscriptFY23Q4\"\n",
    "symbol, fiscal_year, fiscal_quarter = extract_info_from_filename(filename)\n",
    "\n",
    "if symbol and fiscal_year and fiscal_quarter:\n",
    "    print(f\"Symbol: {symbol}\")\n",
    "    print(f\"Fiscal Year: {fiscal_year}\")\n",
    "    print(f\"Fiscal Quarter: {fiscal_quarter}\")\n",
    "else:\n",
    "    print(\"Unable to extract information from the filename.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_line_page_tuples(result):\n",
    "    '''\n",
    "    Input: result of form recognizer analyze_pdf function\n",
    "    Output: Create list of tuples of the form (line, page_num, line_num) \n",
    "    This will keep reference of the line number and page number of each line in the document.\n",
    "    '''\n",
    "    line_page_tuples = []\n",
    "\n",
    "    total_pages = len(result.pages)\n",
    "    for page_num in range(total_pages):\n",
    "        lines = result.pages[page_num].lines\n",
    "        total_lines = len(lines)\n",
    "\n",
    "        for line_num in range(total_lines):\n",
    "            line = lines[line_num].content\n",
    "            line_page_tuples.append((line, page_num + 1, line_num + 1))\n",
    "\n",
    "    return line_page_tuples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_with_page_number(line_page_tuples, chunk_length = 10, chunk_overlap = 2):\n",
    "    '''\n",
    "    Given the list of tuples of the form (line, page_num, line_num) and chunk length and overlap,\n",
    "    it will create chunks of text with page number and line number of the first line in the chunk.\n",
    "    chunk length: number of lines in each chunk\n",
    "    chunk_overlap: number of overlapping lines between chunks\n",
    "    '''\n",
    "    pointer = 0 \n",
    "    chunks = []\n",
    "    total_lines = len(line_page_tuples)\n",
    "    #for line, page_number, line_number in line_page_tuples:\n",
    "    while pointer<total_lines:\n",
    "        line_count =0\n",
    "        current_chunk = \"\"\n",
    "        if not chunks: \n",
    "            # for first chunk we can not use overlap\n",
    "            pointer = 0\n",
    "        else:\n",
    "            pointer = pointer - chunk_overlap\n",
    "        \n",
    "        # take starting page number and line number \n",
    "        page_number, line_number = line_page_tuples[pointer][1:]  \n",
    "        while line_count<chunk_length and pointer<total_lines:\n",
    "            current_chunk = current_chunk + line_page_tuples[pointer][0]\n",
    "            current_chunk = current_chunk + \" \"\n",
    "            line_count += 1\n",
    "            pointer += 1\n",
    "        chunks.append((current_chunk, page_number, line_number))\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read pdf and extract symbol and quarter from file name \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "\n",
    "doc_dir = \"..\\DATA\"\n",
    "pdf_files = [filename for filename in os.listdir(doc_dir) if filename.endswith('.pdf')]\n",
    "#DocId = 0 \n",
    "for file_name in pdf_files:\n",
    "    values = extract_info_from_filename(file_name)\n",
    "    file_path = os.path.join(doc_dir, f\"{os.path.splitext(file_name)[0]}.pdf\")\n",
    "    # analyze the pdf using form recognizer\n",
    "    result = analyze_pdf(file_path)\n",
    "    # get the chunks in a tuple of the form (chunk, page_number, line_number)\n",
    "    line_page_tuples = create_line_page_tuples(result)\n",
    "    chunks = chunk_with_page_number(line_page_tuples = line_page_tuples, chunk_length = 10, chunk_overlap = 2)\n",
    "    #DocId += 1   \n",
    "    \n",
    "    # write the chunks to another csv file \n",
    "    df_chunks = pd.DataFrame(chunks, columns = ['chunk', 'page_number', 'line_number'])  \n",
    "    #df_chunks['DocId'] = DocId\n",
    "    if values:\n",
    "        symbol, fiscal_year, fiscal_quarter = values\n",
    "        df_chunks[\"Ticker\"] = symbol\n",
    "        df_chunks[\"Year\"] = fiscal_year\n",
    "        df_chunks[\"Quarter\"] = fiscal_quarter\n",
    "    else:\n",
    "        df_chunks[\"Ticker\"] = \"NULL\"\n",
    "        df_chunks[\"Year\"] = \"NULL\"\n",
    "        df_chunks[\"Quarter\"] = \"NULL\"\n",
    "    if not os.path.exists(\"../AnalyzedPDF/\"):\n",
    "        os.makedirs(\"../AnalyzedPDF/\")\n",
    "\n",
    "    print('writing the results of: \\n' + file_name)  \n",
    "    if not os.path.exists(f\"../AnalyzedPDF/Chunks_{file_name}.csv\"):\n",
    "        df_chunks.to_csv(f\"../AnalyzedPDF/Chunks_{file_name}.csv\", index=False)\n",
    "    else:\n",
    "        print(f'File: chunks_{file_name}.csv already exists, skipping...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv files in anlyzedPDF folder and create a unified dataframe\n",
    "import pandas as pd\n",
    "import os \n",
    "def read_csv_files(path= \"../AnalyzedPDF/\"):\n",
    "    df = pd.DataFrame()\n",
    "    for file in os.listdir(path):\n",
    "        file_path = os.path.join(path, file)\n",
    "        df = pd.concat(pd.read_csv(file_path), ignore_index=True, axis = 0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd \n",
    "def concatenate_csv_files(path = \"../AnalyzedPDF\"):\n",
    "    csv_files = [file for file in os.listdir(path) if file.endswith(\".csv\")]\n",
    "    dfs = []\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(path, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        dfs.append(df)\n",
    "    print(\"Concatenated files:\", dfs)\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine all the csv files and store in a combined file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_combined_df = concatenate_csv_files(path = \"../AnalyzedPDF\")\n",
    "print(result_combined_df)\n",
    "if not os.path.exists(\"../AnalyzedPDF/CombinedResults\"):\n",
    "    os.makedirs(\"../AnalyzedPDF/CombinedResults\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's rename columns, add index, and save the results\n",
    "result_combined_df[\"Id\"] = result_combined_df.index +1 \n",
    "\n",
    "# now let's add a unique id\n",
    "columns = ['Id'] + [col for col in result_combined_df.columns if col != 'Id']\n",
    "result_combined_df = result_combined_df.reindex(columns=columns)\n",
    "# rename for consistency\n",
    "result_combined_df.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder the columns\n",
    "# Reorder columns\n",
    "new_column_order = ['Id', 'Ticker', 'Year', 'Quarter', 'chunk', 'page_number', 'line_number']\n",
    "result_combined_df_reordered = result_combined_df[new_column_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_filter_id1_df.to_csv(f\"../AnalyzedPDF/CombinedResults/{filter_id1_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Id', 'Ticker', 'Year', 'Quarter', 'chunk', 'page_number', 'line_number'\n",
    "new_columns = {\n",
    "    'Id': \"Id\",\n",
    "    \"Ticker\": \"Ticker\",\n",
    "    \"Year\" : \"Year\",\n",
    "    \"Quarter\": \"Quarter\",\n",
    "    'chunk': 'Chunk',\n",
    "    'page_number': 'PageNumber',\n",
    "    'line_number': 'LineNumber',\n",
    "}\n",
    "result_combined_df_reordered = result_combined_df_reordered.rename(columns=new_columns)\n",
    "# Print the DataFrame with 'Id' as the first column after index\n",
    "result_combined_df_reordered.head(1000)\n",
    "result_combined_df_reordered.to_csv(\"../AnalyzedPDF/CombinedResults/Chunks.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_combined_df_reordered.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
