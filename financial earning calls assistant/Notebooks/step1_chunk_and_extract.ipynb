{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "\n",
    "# specify the name of the .env file name \n",
    "env_name = \"llm_pgvector.env\" # change to your own .env file name\n",
    "config = dotenv_values(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract data and context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to extract text from pdf \n",
    "\"\"\"\n",
    "This code sample shows Prebuilt Document operations with the Azure Form Recognizer client library. \n",
    "The async versions of the samples require Python 3.6 or later.\n",
    "\n",
    "To learn more, please visit the documentation - Quickstart: Form Recognizer Python client library SDKs\n",
    "https://docs.microsoft.com/en-us/azure/applied-ai-services/form-recognizer/quickstarts/try-v3-python-sdk\n",
    "\"\"\"\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "\n",
    "\"\"\"\n",
    "Remember to remove the key from your code when you're done, and never post it publicly. For production, use\n",
    "secure methods to store and access your credentials. For more information, see \n",
    "https://docs.microsoft.com/en-us/azure/cognitive-services/cognitive-services-security?tabs=command-line%2Ccsharp#environment-variables-and-application-configuration\n",
    "\"\"\"\n",
    "\n",
    "endpoint = config[\"AZURE_FORM_RECOGNIZER_ENDPOINT\"]\n",
    "key = config[\"AZURE_FORM_RECOGNIZER_KEY\"]\n",
    "\n",
    "# sample document\n",
    "document_analysis_client = DocumentAnalysisClient(\n",
    "        endpoint=endpoint, credential=AzureKeyCredential(key)\n",
    "    )\n",
    "\n",
    "\n",
    "def analyze_pdf(doc_path):  \n",
    "    with open(doc_path, \"rb\") as f:\n",
    "        poller = document_analysis_client.begin_analyze_document(\n",
    "            \"prebuilt-document\", document=f\n",
    "        )\n",
    "    result = poller.result()\n",
    "    for kv_pair in result.key_value_pairs:\n",
    "\n",
    "        if filter_id1_name in kv_pair.key.content:\n",
    "            print(30*\"*\")\n",
    "            print(\"filter_id1_val:\")\n",
    "            print(\":\\n\")\n",
    "            print(kv_pair.value.content)\n",
    "            print(30*\"*\")\n",
    "                \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Extract value if explicity mentioned in a text. You may develop the template for specific use case\n",
    "extract_name1 = \"Ticker\"  \n",
    "extract_name2= \"Quarter\"\n",
    "def extract_values(text, target =[\"FY\"]):\n",
    "    '''\n",
    "    given a text string, extract the client code and line of business\n",
    "    '''\n",
    "    extract_name1_pattern = r\"{extract_name1}\\s+([\\w,]+)\"\n",
    "    extract_name2_pattern = r\"{extract_name2}\\s+([\\w\\s]+)\"\n",
    "\n",
    "    extract_name1_match = re.search(extract_name1_pattern, text)\n",
    "    extract_name1_match = re.search(extract_name2_pattern, text)\n",
    "\n",
    "    if extract_name1_match and extract_name1_match:\n",
    "        extract_name1 = filter_id1_match.group(1).split(',')  # Split by commas to get a list\n",
    "        extract_name2 = filter_id2_match.group(1).split()[0]  # Extract only the first word\n",
    "        return extract_name1, extract_name2\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbol: MSFT\n",
      "Fiscal Year: 23\n",
      "Fiscal Quarter: 4\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# extract information form the filename\n",
    "def extract_info_from_filename(filename):\n",
    "    pattern = r'([A-Z]+)TranscriptFY(\\d{2})Q(\\d)'\n",
    "    match = re.search(pattern, filename)\n",
    "    \n",
    "    if match:\n",
    "        symbol = match.group(1)\n",
    "        fiscal_year = match.group(2)\n",
    "        fiscal_quarter = match.group(3)\n",
    "        return symbol, fiscal_year, fiscal_quarter\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# example\n",
    "filename = \"MSFTTranscriptFY23Q4\"\n",
    "symbol, fiscal_year, fiscal_quarter = extract_info_from_filename(filename)\n",
    "\n",
    "if symbol and fiscal_year and fiscal_quarter:\n",
    "    print(f\"Symbol: {symbol}\")\n",
    "    print(f\"Fiscal Year: {fiscal_year}\")\n",
    "    print(f\"Fiscal Quarter: {fiscal_quarter}\")\n",
    "else:\n",
    "    print(\"Unable to extract information from the filename.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_line_page_tuples(result):\n",
    "    '''\n",
    "    Input: result of form recognizer analyze_pdf function\n",
    "    Output: Create list of tuples of the form (line, page_num, line_num) \n",
    "    This will keep reference of the line number and page number of each line in the document.\n",
    "    '''\n",
    "    line_page_tuples = []\n",
    "\n",
    "    total_pages = len(result.pages)\n",
    "    for page_num in range(total_pages):\n",
    "        lines = result.pages[page_num].lines\n",
    "        total_lines = len(lines)\n",
    "\n",
    "        for line_num in range(total_lines):\n",
    "            line = lines[line_num].content\n",
    "            line_page_tuples.append((line, page_num + 1, line_num + 1))\n",
    "\n",
    "    return line_page_tuples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_with_page_number(line_page_tuples, chunk_length = 10, chunk_overlap = 2):\n",
    "    '''\n",
    "    Given the list of tuples of the form (line, page_num, line_num) and chunk length and overlap,\n",
    "    it will create chunks of text with page number and line number of the first line in the chunk.\n",
    "    chunk length: number of lines in each chunk\n",
    "    chunk_overlap: number of overlapping lines between chunks\n",
    "    '''\n",
    "    pointer = 0 \n",
    "    chunks = []\n",
    "    total_lines = len(line_page_tuples)\n",
    "    #for line, page_number, line_number in line_page_tuples:\n",
    "    while pointer<total_lines:\n",
    "        line_count =0\n",
    "        current_chunk = \"\"\n",
    "        if not chunks: \n",
    "            # for first chunk we can not use overlap\n",
    "            pointer = 0\n",
    "        else:\n",
    "            pointer = pointer - chunk_overlap\n",
    "        \n",
    "        # take starting page number and line number \n",
    "        page_number, line_number = line_page_tuples[pointer][1:]  \n",
    "        while line_count<chunk_length and pointer<total_lines:\n",
    "            current_chunk = current_chunk + line_page_tuples[pointer][0]\n",
    "            current_chunk = current_chunk + \" \"\n",
    "            line_count += 1\n",
    "            pointer += 1\n",
    "        chunks.append((current_chunk, page_number, line_number))\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read pdf and extract symbol and quarter from file name \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSFT 23 1\n",
      "MSFT 23 2\n",
      "MSFT 23 3\n",
      "MSFT 23 4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "doc_dir = \"..\\DATA\"\n",
    "pdf_files = [filename for filename in os.listdir(doc_dir) if filename.endswith('.pdf')]\n",
    "for file in pdf_files:\n",
    "    symbol, fiscal_year, fiscal_quarter = extract_info_from_filename(file)\n",
    "    print(symbol, fiscal_year, fiscal_quarter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m \n\u001b[0;32m      3\u001b[0m doc_dir \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m..\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mDATA\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m pdf_files \u001b[39m=\u001b[39m [filename \u001b[39mfor\u001b[39;00m filename \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(doc_dir) \u001b[39mif\u001b[39;00m filename\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.pdf\u001b[39m\u001b[39m'\u001b[39m)]\n",
      "File \u001b[1;32mc:\\Users\\hokhadiv\\AppData\\Local\\miniconda3\\envs\\cvs\\lib\\site-packages\\pandas\\__init__.py:48\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[39m# let init-time option registration happen\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconfig_init\u001b[39;00m  \u001b[39m# pyright: ignore # noqa:F401\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     49\u001b[0m     \u001b[39m# dtype\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     ArrowDtype,\n\u001b[0;32m     51\u001b[0m     Int8Dtype,\n\u001b[0;32m     52\u001b[0m     Int16Dtype,\n\u001b[0;32m     53\u001b[0m     Int32Dtype,\n\u001b[0;32m     54\u001b[0m     Int64Dtype,\n\u001b[0;32m     55\u001b[0m     UInt8Dtype,\n\u001b[0;32m     56\u001b[0m     UInt16Dtype,\n\u001b[0;32m     57\u001b[0m     UInt32Dtype,\n\u001b[0;32m     58\u001b[0m     UInt64Dtype,\n\u001b[0;32m     59\u001b[0m     Float32Dtype,\n\u001b[0;32m     60\u001b[0m     Float64Dtype,\n\u001b[0;32m     61\u001b[0m     CategoricalDtype,\n\u001b[0;32m     62\u001b[0m     PeriodDtype,\n\u001b[0;32m     63\u001b[0m     IntervalDtype,\n\u001b[0;32m     64\u001b[0m     DatetimeTZDtype,\n\u001b[0;32m     65\u001b[0m     StringDtype,\n\u001b[0;32m     66\u001b[0m     BooleanDtype,\n\u001b[0;32m     67\u001b[0m     \u001b[39m# missing\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     NA,\n\u001b[0;32m     69\u001b[0m     isna,\n\u001b[0;32m     70\u001b[0m     isnull,\n\u001b[0;32m     71\u001b[0m     notna,\n\u001b[0;32m     72\u001b[0m     notnull,\n\u001b[0;32m     73\u001b[0m     \u001b[39m# indexes\u001b[39;00m\n\u001b[0;32m     74\u001b[0m     Index,\n\u001b[0;32m     75\u001b[0m     CategoricalIndex,\n\u001b[0;32m     76\u001b[0m     RangeIndex,\n\u001b[0;32m     77\u001b[0m     MultiIndex,\n\u001b[0;32m     78\u001b[0m     IntervalIndex,\n\u001b[0;32m     79\u001b[0m     TimedeltaIndex,\n\u001b[0;32m     80\u001b[0m     DatetimeIndex,\n\u001b[0;32m     81\u001b[0m     PeriodIndex,\n\u001b[0;32m     82\u001b[0m     IndexSlice,\n\u001b[0;32m     83\u001b[0m     \u001b[39m# tseries\u001b[39;00m\n\u001b[0;32m     84\u001b[0m     NaT,\n\u001b[0;32m     85\u001b[0m     Period,\n\u001b[0;32m     86\u001b[0m     period_range,\n\u001b[0;32m     87\u001b[0m     Timedelta,\n\u001b[0;32m     88\u001b[0m     timedelta_range,\n\u001b[0;32m     89\u001b[0m     Timestamp,\n\u001b[0;32m     90\u001b[0m     date_range,\n\u001b[0;32m     91\u001b[0m     bdate_range,\n\u001b[0;32m     92\u001b[0m     Interval,\n\u001b[0;32m     93\u001b[0m     interval_range,\n\u001b[0;32m     94\u001b[0m     DateOffset,\n\u001b[0;32m     95\u001b[0m     \u001b[39m# conversion\u001b[39;00m\n\u001b[0;32m     96\u001b[0m     to_numeric,\n\u001b[0;32m     97\u001b[0m     to_datetime,\n\u001b[0;32m     98\u001b[0m     to_timedelta,\n\u001b[0;32m     99\u001b[0m     \u001b[39m# misc\u001b[39;00m\n\u001b[0;32m    100\u001b[0m     Flags,\n\u001b[0;32m    101\u001b[0m     Grouper,\n\u001b[0;32m    102\u001b[0m     factorize,\n\u001b[0;32m    103\u001b[0m     unique,\n\u001b[0;32m    104\u001b[0m     value_counts,\n\u001b[0;32m    105\u001b[0m     NamedAgg,\n\u001b[0;32m    106\u001b[0m     array,\n\u001b[0;32m    107\u001b[0m     Categorical,\n\u001b[0;32m    108\u001b[0m     set_eng_float_format,\n\u001b[0;32m    109\u001b[0m     Series,\n\u001b[0;32m    110\u001b[0m     DataFrame,\n\u001b[0;32m    111\u001b[0m )\n\u001b[0;32m    113\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39marrays\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msparse\u001b[39;00m \u001b[39mimport\u001b[39;00m SparseDtype\n\u001b[0;32m    115\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtseries\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m infer_freq\n",
      "File \u001b[1;32mc:\\Users\\hokhadiv\\AppData\\Local\\miniconda3\\envs\\cvs\\lib\\site-packages\\pandas\\core\\api.py:27\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdtypes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmissing\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m     isna,\n\u001b[0;32m     17\u001b[0m     isnull,\n\u001b[0;32m     18\u001b[0m     notna,\n\u001b[0;32m     19\u001b[0m     notnull,\n\u001b[0;32m     20\u001b[0m )\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39malgorithms\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m     factorize,\n\u001b[0;32m     24\u001b[0m     unique,\n\u001b[0;32m     25\u001b[0m     value_counts,\n\u001b[0;32m     26\u001b[0m )\n\u001b[1;32m---> 27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39marrays\u001b[39;00m \u001b[39mimport\u001b[39;00m Categorical\n\u001b[0;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39marrays\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39marrow\u001b[39;00m \u001b[39mimport\u001b[39;00m ArrowDtype\n\u001b[0;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39marrays\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mboolean\u001b[39;00m \u001b[39mimport\u001b[39;00m BooleanDtype\n",
      "File \u001b[1;32mc:\\Users\\hokhadiv\\AppData\\Local\\miniconda3\\envs\\cvs\\lib\\site-packages\\pandas\\core\\arrays\\__init__.py:21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39marrays\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msparse\u001b[39;00m \u001b[39mimport\u001b[39;00m SparseArray\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39marrays\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstring_\u001b[39;00m \u001b[39mimport\u001b[39;00m StringArray\n\u001b[1;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39marrays\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstring_arrow\u001b[39;00m \u001b[39mimport\u001b[39;00m ArrowStringArray\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39marrays\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtimedeltas\u001b[39;00m \u001b[39mimport\u001b[39;00m TimedeltaArray\n\u001b[0;32m     24\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[0;32m     25\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mArrowExtensionArray\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     26\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mExtensionArray\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mTimedeltaArray\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     43\u001b[0m ]\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:879\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:969\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1091\u001b[0m, in \u001b[0;36mpath_stats\u001b[1;34m(self, path)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "doc_dir = \"..\\DATA\"\n",
    "pdf_files = [filename for filename in os.listdir(doc_dir) if filename.endswith('.pdf')]\n",
    "DocId = 0 \n",
    "for file in pdf_files:\n",
    "    values = extract_info_from_filename(file)\n",
    "    file_path = os.path.join(doc_dir, f\"{os.path.splitext(file)[0]}.pdf\")\n",
    "    # analyze the pdf using form recognizer\n",
    "    result = analyze_pdf(file_path)\n",
    "    # get the chunks in a tuple of the form (chunk, page_number, line_number)\n",
    "    line_page_tuples = create_line_page_tuples(result)\n",
    "    chunks = chunk_with_page_number(line_page_tuples = line_page_tuples, chunk_length = 10, chunk_overlap = 2)\n",
    "    DocId += 1   \n",
    "    \n",
    "    # write the chunks to another csv file \n",
    "    df_chunks = pd.DataFrame(chunks, columns = ['chunk', 'page_number', 'line_number'])  \n",
    "    df_chunks['DocId'] = DocId\n",
    "    if values:\n",
    "        symbol, fiscal_year, fiscal_quarter = values\n",
    "        df_chunks[\"Ticker\"] = symbol\n",
    "        df_chunks[\"Year\"] = fiscal_year\n",
    "        df_chunks[\"Quarter\"] = fiscal_quarter\n",
    "    else:\n",
    "        df_chunks[\"Ticker\"] = \"NULL\"\n",
    "        df_chunks[\"Year\"] = \"NULL\"\n",
    "        df_chunks[\"Quarter\"] = \"NULL\"\n",
    "    if not os.path.exists(\"../AnalyzedPDF/Chunks/\"):\n",
    "        os.makedirs(\"../AnalyzedPDF/Chunks\")\n",
    "\n",
    "print('writing the results of: \\n' + file)  \n",
    "    if not os.path.exists(f\"../AnalyzedPDF/Chunks/Chunks_{file}\"+\".csv\"):\n",
    "        df_chunks.to_csv(\"../AnalyzedPDF/Chunks/\" + 'DocId_'+ str(DocId) + \".csv\", index=False)\n",
    "    else:\n",
    "        print(f'File: chunks_{file}.csv already exists, skipping...')\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv files in anlyzedPDF2 folder and create a dataframe\n",
    "import pandas as pd\n",
    "import os \n",
    "def read_csv_files(path= \"../AnalyzedPDF\"):\n",
    "    df = pd.DataFrame()\n",
    "    for file in os.listdir(path):\n",
    "        file_path = os.path.join(path, file)\n",
    "        df = pd.concat(pd.read_csv(file_path), ignore_index=True, axis = 0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd \n",
    "def concatenate_csv_files(path = \"../AnalyzedPDF\"):\n",
    "    csv_files = [file for file in os.listdir(path) if file.endswith(\".csv\")]\n",
    "    dfs = []\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(path, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        dfs.append(df)\n",
    "    print(\"Concatenated files:\", dfs)\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine all the csv files for filter_id and chunks and store in separate combined csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = f\"../AnalyzedPDF/{filter_id1_name}\"\n",
    "result_filter_id1_df = concatenate_csv_files(folder_path)\n",
    "print(result_filter_id1_df)\n",
    "if not os.path.exists(\"../AnalyzedPDF/CombinedResults\"):\n",
    "    os.makedirs(\"../AnalyzedPDF/CombinedResults\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's rename columns, add index, and save the results\n",
    "result_filter_id1_df[\"Id\"] = result_filter_id1_df.index +1 \n",
    "\n",
    "# now let's add a unique id\n",
    "columns = ['Id'] + [col for col in result_filter_id1_df.columns if col != 'Id']\n",
    "result_filter_id1_df = result_filter_id1_df.reindex(columns=columns)\n",
    "# rename for consistency\n",
    "result_filter_id1_df.columns = ['Id', 'DocId', filter_id1_name, filter_id2_name]\n",
    "result_filter_id1_df.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_filter_id1_df.to_csv(f\"../AnalyzedPDF/CombinedResults/{filter_id1_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"../AnalyzedPDF/Chunks\"\n",
    "result_chunk_df = concatenate_csv_files(folder_path)\n",
    "print(result_chunk_df)\n",
    "# add primary key\n",
    "result_chunk_df[\"Id\"] = result_chunk_df.index +1    \n",
    "#make Id as the first column as it will be used as primary key\n",
    "columns = ['Id'] + [col for col in result_chunk_df.columns if col != 'Id']\n",
    "result_chunk_df = result_chunk_df.reindex(columns=columns)\n",
    "new_columns = {\n",
    "    'Id': \"Id\",\n",
    "    'chunk': 'Chunk',\n",
    "    'Embedding': 'Embedding',\n",
    "    'page_number': 'PageNumber',\n",
    "    'DocID': 'DocID',\n",
    "    'line_number': 'LineNumber',\n",
    "}\n",
    "result_chunk_df = result_chunk_df.rename(columns=new_columns)\n",
    "# Print the DataFrame with 'Id' as the first column after index\n",
    "result_chunk_df.head(1000)\n",
    "result_chunk_df.to_csv(\"../AnalyzedPDF/CombinedResults/Chunks.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_chunk_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_filter_id1_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
