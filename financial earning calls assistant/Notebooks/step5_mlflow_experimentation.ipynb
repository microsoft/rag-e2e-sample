{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load configs for database, Azure OpenAI, and other resources as environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "\n",
    "# specify the name of the .env file name \n",
    "env_name = \"llm_pgvector.env\" # change to your own .env file name\n",
    "config = dotenv_values(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Flex Postgres (PG)  for retrievals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection pool created successfully\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import pool\n",
    "from psycopg2 import Error\n",
    "\n",
    "host = config[\"HOST\"]\n",
    "dbname = config[\"DBNAME\"] \n",
    "user = config[\"USER\"] \n",
    "password = config[\"PASSWORD\"] \n",
    "sslmode = config[\"SSLMODE\"] \n",
    "\n",
    "# Build a connection string from the variables\n",
    "conn_string = \"host={0} user={1} dbname={2} password={3} sslmode={4}\".format(host, user, dbname, password, sslmode)\n",
    "\n",
    "postgreSQL_pool = psycopg2.pool.SimpleConnectionPool(1, 20,conn_string)\n",
    "if (postgreSQL_pool):\n",
    "    print(\"Connection pool created successfully\")\n",
    "\n",
    "# Use getconn() to get a connection from the connection pool\n",
    "connection = postgreSQL_pool.getconn()\n",
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare for question embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "openai.api_type = config[\"OPENAI_API_TYPE\"] \n",
    "openai.api_key = config['OPENAI_API_KEY']\n",
    "openai.api_base = config['OPENAI_API_BASE'] \n",
    "openai.api_version = config['OPENAI_API_VERSION']  \n",
    "\n",
    "\n",
    "def createEmbeddings(text):\n",
    "    response = openai.Embedding.create(input=text , engine=config[\"OPENAI_DEPLOYMENT_EMBEDDING\"])\n",
    "    embeddings = response['data'][0]['embedding']\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define llm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai.api_key\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://synapseml-openai.openai.azure.com/\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2022-12-01\"\n",
    "os.environ[\"OPENAI_DEPLOYMENT_NAME\"] = \"text-davinci-003\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import AzureOpenAI\n",
    "llm= AzureOpenAI(deployment_name=config[\"OPENAI_MODEL_COMPLETION\"], model_name=config[\"OPENAI_MODEL_EMBEDDING\"], temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders.base import BaseLoader\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "class TextFormatter(BaseLoader):\n",
    "    \"\"\"Load text files.\"\"\"\n",
    "\n",
    "    def __init__(self, text: str):\n",
    "        \"\"\"Initialize with file path.\"\"\"\n",
    "        self.text = text\n",
    "\n",
    "    def load(self) -> List[Document]:\n",
    "        \"\"\"Load from file path.\"\"\"\n",
    "        metadata = {\"source\": \"\"}\n",
    "        return [Document(page_content=self.text, metadata=metadata)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Navigate to the directory containing the CSV file (one level above the current directory)\n",
    "data_directory = os.path.abspath(os.path.join(current_directory, '..', 'ValidationSetOfQA'))\n",
    "\n",
    "# Construct the file path for your CSV file in the data_directory\n",
    "csv_file_path = os.path.join(data_directory, 'QnAValidationSet.csv')\n",
    "\n",
    "# Load the CSV file using pandas\n",
    "df = pd.read_csv(csv_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Ticker', 'Quarter', 'Question', 'Answer', 'Page'], dtype='object')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the filters you want to apply \n",
    "filter1_name = 'Ticker'\n",
    "filter2_name = 'Quarter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Quarter</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>1</td>\n",
       "      <td>How did the first quarter financial results co...</td>\n",
       "      <td>In the first quarter, our revenue reached $50....</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>1</td>\n",
       "      <td>What were the key highlights and growth trends...</td>\n",
       "      <td>Within the commercial business segment, we wit...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Ticker  Quarter                                           Question  \\\n",
       "0   MSFT        1  How did the first quarter financial results co...   \n",
       "1   MSFT        1  What were the key highlights and growth trends...   \n",
       "\n",
       "                                              Answer  Page  \n",
       "0  In the first quarter, our revenue reached $50....    18  \n",
       "1  Within the commercial business segment, we wit...    19  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval = df.copy()\n",
    "df_eval.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [item for pair in zip(list(df_eval['Question']), list(df_eval['Answer'])) for item in pair]\n",
    "keys = [str(i//2)+'a' if i%2==0 else str(i//2+1)+'q' for i in range(1,len(values)+2)]\n",
    "\n",
    "userQuestions = {keys[i]:values[i] for i in range(len(keys)-1)}\n",
    "filter1_vals = [item for item in list(df_eval[filter1_name]) for _ in range(2)]\n",
    "filter2_vals = [item for item in list(df_eval[filter2_name]) for _ in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT based question answering with type checking\n",
    "from langchain import PromptTemplate\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_similarities(QA_results, verbose = False):\n",
    "    # compare cosine similarity between two vectors\n",
    "    cosine_similarities = []\n",
    "    for i in range(len(QA_results[0])):\n",
    "        if verbose:\n",
    "            print('calculating cosine similarity for: \\n', QA_results[0][i], '\\n', QA_results[1][i])\n",
    "            print(30*'-')\n",
    "        emd1 = createEmbeddings(QA_results[0][i])\n",
    "        emd2 = createEmbeddings(QA_results[1][i])\n",
    "        cosine_similarity_val = cosine_similarity(\n",
    "            np.array(emd1).reshape(1, -1), np.array(emd2).reshape(1, -1)\n",
    "        )[0][0]\n",
    "        cosine_similarities.append(np.round(cosine_similarity_val, 2))\n",
    "    return cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's also evaluate the reference page number\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_page_number(text):\n",
    "    # Regular expression pattern to find the PageNumber value\n",
    "    pattern = r'PageNumber:\\s+(\\d+)'\n",
    "\n",
    "    # Search for the pattern in the text\n",
    "    match = re.search(pattern, text)\n",
    "\n",
    "    # If a match is found, return the extracted PageNumber value, otherwise return 0\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_all_page_numbers(Agpt):\n",
    "    page_numbers = []\n",
    "    for answers in Agpt:\n",
    "        page_numbers.append(extract_page_number(answers))\n",
    "    print(page_numbers)\n",
    "    print(df_eval[\"PageNumber\"].tolist())\n",
    "    page_number_score = [1 if page_numbers[i] == df_eval[\"PageNumber\"].tolist()[i] else 0 for i in range(len(page_numbers))]\n",
    "    print(page_number_score)\n",
    "    return page_numbers, page_number_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgvector.psycopg2 import register_vector\n",
    "\n",
    "def retrieve_k_chunk(retrieve_k, questionEmbedding,filter1_val, filter2_val, similarity_method, verbose=False):\n",
    "\n",
    "    connection = psycopg2.connect(conn_string)\n",
    "    # Create a cursor after the connection\n",
    "    # Register 'pgvector' type for the 'embedding' column\n",
    "    register_vector(connection)\n",
    "    print(\"filter1_name:\", filter1_name)\n",
    "    select_query = f\"SELECT Id FROM {table_name} where {filter1_name} = '{filter1_val}' and {filter2_name}='{filter2_val}' ORDER BY embedding <-> %s LIMIT {retrieve_k}\"\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(select_query, (np.array(questionEmbedding),))\n",
    "    results = cursor.fetchall()\n",
    "    top_ids = []\n",
    "    for i in range(len(results)):\n",
    "        top_ids.append(int(results[i][0]))\n",
    "\n",
    "    # Rollback the current transaction\n",
    "    connection.rollback()\n",
    "\n",
    "    format_ids = ', '.join(['%s'] * len(top_ids))\n",
    "\n",
    "    sql = f\"SELECT CONCAT('PageNumber: ', PageNumber, ' ', 'LineNumber: ', LineNumber, ' ', 'Text: ', Chunk) AS concat FROM {table_name} WHERE id IN ({format_ids})\"\n",
    "    # Execute the SELECT statement\n",
    "    try:\n",
    "        cursor.execute(sql, top_ids)    \n",
    "        top_rows = cursor.fetchall()\n",
    "    except (Exception, Error) as e:\n",
    "        print(f\"Error executing SELECT statement: {e}\")\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        \n",
    "    sql_pages = f\"SELECT PageNumber FROM {table_name} WHERE id IN ({format_ids})\"\n",
    "    # Execute the SELECT statement\n",
    "    cursor = connection.cursor()\n",
    "    try:\n",
    "        cursor.execute(sql_pages, top_ids)    \n",
    "        pages = cursor.fetchall()\n",
    "    except (Exception, Error) as e:\n",
    "        print(f\"Error executing SELECT statement: {e}\")\n",
    "    finally:\n",
    "        cursor.close()  \n",
    "  \n",
    "    print(pages)\n",
    "    retrieved_pages = [int(page[0]) for page in pages]\n",
    "    return top_rows, retrieved_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_questions_answers(retrieve_k, filter1_vals, filter2_vals, similarity_method, QUESTION_PROMPT, verbose=False):\n",
    "    \"\"\"\n",
    "    Collection of user questions with known answers.\n",
    "    \"\"\"\n",
    "    Q = []\n",
    "    A = []\n",
    "    Agpt = []\n",
    "    contexts = []\n",
    "    pages = []\n",
    "    i = 0\n",
    "    for key, value in userQuestions.items():\n",
    "        if \"q\" in key:\n",
    "            Q.append(value)\n",
    "            questionEmbedding = createEmbeddings(value)\n",
    "            if verbose:\n",
    "                print(\"question: \", value)\n",
    "            top_rows, retreived_pages = retrieve_k_chunk(retrieve_k, questionEmbedding, filter1_vals[i], filter2_vals[i], similarity_method, verbose = verbose)\n",
    "            # create the context from the top_rows\n",
    "            context = \"\"\n",
    "            for row in top_rows:\n",
    "                context += row[0]\n",
    "                context += \"\\n\"\n",
    "            if verbose:\n",
    "                print('context: \\n', context)\n",
    "            loader = TextFormatter(context)\n",
    "            chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=QUESTION_PROMPT)\n",
    "            ans = chain({\"input_documents\": loader.load(), \"question\": value}, return_only_outputs=True)\n",
    "            Agpt.append(ans['output_text'])\n",
    "            contexts.append(context)\n",
    "            pages.append(retreived_pages)\n",
    "            if verbose:\n",
    "                print(ans['output_text'])\n",
    "            i+=2\n",
    "        else:\n",
    "            A.append(value)\n",
    "        \n",
    "    return  Q, A, Agpt, contexts, pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_text(config_dict):\n",
    "    config_text = \"\"\n",
    "    for key, value in config_dict.items():\n",
    "        config_text += f\"{key}: {value}\\n\"\n",
    "    return config_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a filename with the timestamp as part of the extension\n",
    "# You can choose any file name with an appropriate extension (.csv, .parquet, etc.)\n",
    "\n",
    "def run_experiment(ExperimentConfig, QUESTION_PROMPT, verbose=False):\n",
    "    config_text = dict_to_text(ExperimentConfig)\n",
    "    Q, A, Agpt, contexts, pages = get_user_questions_answers(retrieve_k = ExperimentConfig['retrieve_k'] ,filter1_vals=filter1_vals, filter2_vals = filter2_vals, similarity_method = ExperimentConfig['similarity_method'], QUESTION_PROMPT=QUESTION_PROMPT, verbose=verbose)\n",
    "    QAres = [A, Agpt, Q]\n",
    "    cos_sim_scores = get_cosine_similarities(QAres, verbose= True)\n",
    "    page_numbers, page_number_score = get_all_page_numbers(Agpt)\n",
    "    df_evaluation = pd.DataFrame({'Question': Q, 'Answer': A, 'Answer_gpt': Agpt, 'Score': cos_sim_scores, 'detected_page_number': page_numbers, 'actual_page_number': df_eval[\"PageNumber\"].tolist(), 'page_number_score': page_number_score, 'context': contexts, \\\n",
    "        'retrieved_pages': pages})\n",
    "    df_evaluation[\"correct_page_in_retrieved\"] = df_evaluation.apply(lambda row: row['actual_page_number'] in row['retrieved_pages'], axis=1)\n",
    "    df_evaluation[\"config\"] = config_text\n",
    "    df_evaluation[\"prompt\"] = QUESTION_PROMPT.template\n",
    "    current_timestamp = pd.Timestamp.now()\n",
    "    timestamp_str = current_timestamp.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    file_path ='..\\DATA\\evaluation'+ '_retrieve_'+ str(ExperimentConfig['retrieve_k']) + '_similarity_'+ ExperimentConfig['similarity_method']+'_date_' +timestamp_str + '_.csv'  \n",
    "    df_evaluation.to_csv(file_path, index=False)\n",
    "    return np.mean(cos_sim_scores), np.mean(page_number_score),  cos_sim_scores, page_number_score, df_evaluation, file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation using MLFLOW library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run multiple experiments using mlflow to compare results of various search methods, and number of top retrievals. Similarly, other parameter variation based experiments can be run and tracked using mlflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter1_name: Ticker\n",
      "[(1,), (18,), (21,), (24,), (36,)]\n",
      "filter1_name: Ticker\n",
      "[(3,), (19,), (22,), (22,), (29,)]\n",
      "calculating cosine similarity for: \n",
      " In the first quarter, our revenue reached $50.1 billion, representing an 11 percent increase or 16 percent growth when adjusted for constant currency. Earnings per share stood at $2.35, showing a 4 percent increase or 11 percent growth in constant currency, after accounting for the net tax benefit from the previous year's first quarter. \n",
      " \n",
      "\n",
      "Answer: The first quarter revenue was $50.1 billion, up 11 percent and 16 percent in constant currency (Page 18, Line 8). Earnings per share was $2.35 - and increased 4 percent and 11 percent in constant currency, when adjusted for the net tax benefit from the first quarter of fiscal year 22 (Page 18, Line 8).\n",
      "------------------------------\n",
      "calculating cosine similarity for: \n",
      " Within the commercial business segment, we witnessed strong demand for our Microsoft Cloud offerings, resulting in a remarkable 31 percent growth in constant currency. Moreover, we achieved share gains across multiple businesses. While commercial bookings experienced a 3 percent decline, they increased by 16 percent in constant currency on a flat expiry base. This growth was driven by robust renewal execution and an increase in large, long-term contracts for Azure and Microsoft 365 across various deal sizes. Notably, more than half of the bookings exceeding $10 million in value came from Microsoft 365 E5. \n",
      " \n",
      "\n",
      "Answer: The key highlights and growth trends observed in the commercial business segment during the first quarter were that Microsoft Cloud offerings exceeded $25 billion in quarterly revenue, up 24 percent and 31 percent in constant currency (Page 3, Line 11). There were also share gains across many businesses (Page 19, Line 9). In terms of commercial bookings, there was a decline of 3 percent and an increase of 16 percent in constant currency on a flat expiry base (Page 19, Line 9). This growth was driven by strong renewal execution and an increase in the number of large, long-term Azure and Microsoft 365 contracts across all deal sizes (Page 19, Line 9). Microsoft 365 drove strong E5 momentum again this quarter, with paid Office 365 commercial seats growing 14 percent year-over-year (Page 22, Line 3). Dynamics 365 also grew 24 percent and 32 percent in constant currency (Page 22, Line 11).\n",
      "------------------------------\n",
      "[0, 0]\n"
     ]
    }
   ],
   "source": [
    "import mlflow, time\n",
    "mlflow.set_experiment(experiment_name=\"RAG_EXP\")\n",
    "table_name = 'EarningsCallChunksEmbedding'\n",
    "ExperimentConfig1 = {'retrieve_k': 5, 'similarity_method': 'knn'} # similarity = ['cosine', 'NN', 'inner']\n",
    "ExperimentConfig2 = {'retrieve_k': 10, 'similarity_method': 'knn'} # similarity = ['cosine', 'NN', 'inner']\n",
    "ExperimentConfig3 = {'retrieve_k': 2, 'similarity_method': 'knn'} # similarity = ['cosine', 'NN', 'inner']\n",
    "ExperimentConfig4 = {'retrieve_k': 5, 'similarity_method': 'cosine'} # similarity = ['cosine', 'NN', 'inner']\n",
    "ExperimentConfig5 = {'retrieve_k': 5, 'similarity_method': 'inner'} # similarity = ['cosine', 'NN', 'inner']\n",
    "ExperimentConfig6 = {'retrieve_k': 5, 'similarity_method': 'knn'} # similarity = ['cosine', 'NN', 'inner']\n",
    "\n",
    "RUNConfigs = [ExperimentConfig1, ExperimentConfig2, ExperimentConfig3, ExperimentConfig4, ExperimentConfig5]\n",
    "question_prompt_template = \"\"\"Use the following portion of the context document to find relevant text and answer the question in details. Extract PageNumber and LineNumber and show it in the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "If the answer is not found, say that answer is not available in the documentation.\"\"\"\n",
    "QUESTION_PROMPT = PromptTemplate(\n",
    "    template=question_prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "for ExperimentConfig in RUNConfigs:\n",
    "    current_timestamp = pd.Timestamp.now()\n",
    "    timestamp_str = current_timestamp.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    mlflow_run_name = \"Retreive_k_\" + str(ExperimentConfig['retrieve_k']) + \"_similarity_\" + ExperimentConfig['similarity_method']+ \"_date_\" + timestamp_str\n",
    "    with mlflow.start_run(run_name=mlflow_run_name) as run:\n",
    "        mean_sim_score, mean_page_score,cos_sim_score, page_number_score, df_evaluation, df_path = run_experiment(ExperimentConfig, QUESTION_PROMPT, verbose = False)\n",
    "        print(\"mean_sim_score, mean_page_score:\", mean_sim_score, mean_page_score)\n",
    "        mlflow.log_metric(\"mean_sim_score\", mean_sim_score)\n",
    "        mlflow.log_metric(\"mean_page_score\", mean_page_score)\n",
    "        mlflow.log_param(\"cosine_similarity_score\", str(cos_sim_score))\n",
    "        mlflow.log_param(\"page_number_score\", str(page_number_score))\n",
    "        mlflow.log_param(\"Client code table\", table_name1)\n",
    "        mlflow.log_param(\"Chunk table\", table_name2)\n",
    "        mlflow.log_param(\"prompt\", QUESTION_PROMPT)  \n",
    "        mlflow.log_param(\"config\", \"\".join(ExperimentConfig))\n",
    "        mlflow.log_artifact(df_path)\n",
    "        print(\"config :\\n\", ExperimentConfig)\n",
    "        time.sleep(0.5)\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results will be saved in `\\mlruns` inside the `Notebooks` directory. To view visualization, run `mlflow ui` command from the Notebooks directory path level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flexenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
