{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON IO: In this notebook, we will experiment with prompt template. We will pass input context as json and extract output answer from gpt as json format as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load configs for database, Azure OpenAI, and other resources as environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "\n",
    "# specify the name of the .env file name \n",
    "env_name = \"llm_pgvector.env\" # change to your own .env file name\n",
    "config = dotenv_values(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Flex Postgres (PG)  for retrievals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import pool\n",
    "from psycopg2 import Error\n",
    "\n",
    "host = config[\"HOST\"]\n",
    "dbname = config[\"DBNAME\"] \n",
    "user = config[\"USER\"] \n",
    "password = config[\"PASSWORD\"] \n",
    "sslmode = config[\"SSLMODE\"]  \n",
    "\n",
    "# Build a connection string from the variables\n",
    "conn_string = \"host={0} user={1} dbname={2} password={3} sslmode={4}\".format(host, user, dbname, password, sslmode)\n",
    "\n",
    "postgreSQL_pool = psycopg2.pool.SimpleConnectionPool(1, 20,conn_string)\n",
    "if (postgreSQL_pool):\n",
    "    print(\"Connection pool created successfully\")\n",
    "\n",
    "# Use getconn() to get a connection from the connection pool\n",
    "connection = postgreSQL_pool.getconn()\n",
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare for question embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "openai.api_type = config[\"OPENAI_API_TYPE\"] \n",
    "openai.api_key = config['OPENAI_API_KEY']\n",
    "openai.api_base = config['OPENAI_API_BASE'] \n",
    "openai.api_version = config['OPENAI_API_VERSION']  \n",
    "\n",
    "\n",
    "def createEmbeddings(text):\n",
    "    response = openai.Embedding.create(input=text , engine=config[\"OPENAI_DEPLOYMENT_EMBEDDING\"])\n",
    "    embeddings = response['data'][0]['embedding']\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define llm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai.api_key\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://synapseml-openai.openai.azure.com/\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2022-12-01\"\n",
    "os.environ[\"OPENAI_DEPLOYMENT_NAME\"] = \"text-davinci-003\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import AzureOpenAI\n",
    "llm= AzureOpenAI(deployment_name=config[\"OPENAI_MODEL_COMPLETION\"], model_name=config[\"OPENAI_MODEL_EMBEDDING\"], temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders.base import BaseLoader\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "class TextFormatter(BaseLoader):\n",
    "    \"\"\"Load text files.\"\"\"\n",
    "\n",
    "    def __init__(self, text: str):\n",
    "        \"\"\"Initialize with file path.\"\"\"\n",
    "        self.text = text\n",
    "\n",
    "    def load(self) -> List[Document]:\n",
    "        \"\"\"Load from file path.\"\"\"\n",
    "        metadata = {\"source\": \"\"}\n",
    "        return [Document(page_content=self.text, metadata=metadata)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Navigate to the directory containing the CSV file (one level above the current directory)\n",
    "data_directory = os.path.abspath(os.path.join(current_directory, '..', 'ValidationSetOfQA'))\n",
    "\n",
    "# Construct the file path for your CSV file in the data_directory\n",
    "csv_file_path = os.path.join(data_directory, 'QnAValidationSet.csv')\n",
    "\n",
    "# Load the CSV file using pandas\n",
    "df = pd.read_csv(csv_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_id1_name = \"\"\n",
    "filter_id2_name = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [filter_id1_name, filter_id2_name, 'Question', 'Answer', 'ReferenceText', 'PageNumber']\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = df.copy()\n",
    "df_eval.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [item for pair in zip(list(df_eval['Question']), list(df_eval['Answer'])) for item in pair]\n",
    "keys = [str(i//2)+'a' if i%2==0 else str(i//2+1)+'q' for i in range(1,len(values)+2)]\n",
    "\n",
    "userQuestions = {keys[i]:values[i] for i in range(len(keys)-1)}\n",
    "filter_id1_vals = [item for item in list(df_eval[filter_id1_name]) for _ in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT based question answering with type checking\n",
    "from langchain import PromptTemplate\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_similarities(QA_results, verbose = False):\n",
    "    # compare cosine similarity between two vectors\n",
    "    cosine_similarities = []\n",
    "    for i in range(len(QA_results[0])):\n",
    "        if verbose:\n",
    "            print('calculating cosine similarity for: \\n', QA_results[0][i], '\\n', QA_results[1][i])\n",
    "            print(30*'-')\n",
    "        emd1 = createEmbeddings(QA_results[0][i])\n",
    "        emd2 = createEmbeddings(QA_results[1][i])\n",
    "        cosine_similarity_val = cosine_similarity(\n",
    "            np.array(emd1).reshape(1, -1), np.array(emd2).reshape(1, -1)\n",
    "        )[0][0]\n",
    "        cosine_similarities.append(np.round(cosine_similarity_val, 2))\n",
    "    return cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's also evaluate the reference page number\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_page_number(text):\n",
    "    # Regular expression pattern to find the PageNumber value\n",
    "    pattern = r'PageNumber:\\s+(\\d+)'\n",
    "\n",
    "    # Search for the pattern in the text\n",
    "    match = re.search(pattern, text)\n",
    "\n",
    "    # If a match is found, return the extracted PageNumber value, otherwise return 0\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_all_page_numbers(Agpt):\n",
    "    page_numbers = []\n",
    "    for answers in Agpt:\n",
    "        page_numbers.append(extract_page_number(answers))\n",
    "    print(page_numbers)\n",
    "    print(df_eval[\"PageNumber\"].tolist())\n",
    "    page_number_score = [1 if page_numbers[i] == df_eval[\"PageNumber\"].tolist()[i] else 0 for i in range(len(page_numbers))]\n",
    "    print(page_number_score)\n",
    "    return page_numbers, page_number_score\n",
    "    \n",
    "    \n",
    "def get_all_page_numbers_from_json(Agpt):\n",
    "    page_numbers = []\n",
    "    for answers in Agpt:\n",
    "        print('page numbers: ', answers[\"Answer\"][\"PageNumber\"])\n",
    "        print(type(answers))\n",
    "        print(type(answers[\"Answer\"][\"PageNumber\"]))\n",
    "        #page_numbers.append(int(answers[\"Answer\"][\"PageNumber\"]))\n",
    "    print(page_numbers)\n",
    "    print(df_eval[\"PageNumber\"].tolist())\n",
    "    page_number_score = [1 if page_numbers[i] == df_eval[\"PageNumber\"].tolist()[i] else 0 for i in range(len(page_numbers))]\n",
    "    print(page_number_score)\n",
    "    return page_numbers, page_number_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgvector.psycopg2 import register_vector\n",
    "\n",
    "def retrieve_k_chunk(retrieve_k, questionEmbedding,filter_id1_val, similarity_method, verbose=False):\n",
    "    connection = psycopg2.connect(conn_string)\n",
    "# Create a cursor after the connection\n",
    "# Register 'pgvector' type for the 'embedding' column\n",
    "    register_vector(connection)\n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    #print(\"filter_id1_name:\", filter_id1_name)\n",
    "    select_docid_query = f\"SELECT DocId FROM {table_name1} WHERE {filter_id1_name} = '{filter_id1_val}'\"\n",
    "    cursor.execute(select_docid_query)\n",
    "    doc_id = cursor.fetchone()[0]\n",
    "    if verbose:\n",
    "        print('filter_id1_name:', filter_id1_name)\n",
    "        print('DocId:', doc_id)\n",
    "    \n",
    "    if similarity_method == 'NN':\n",
    "        sign = '<->'\n",
    "    elif similarity_method=='cosine':\n",
    "        sign = '<=>'\n",
    "    elif similarity_method=='inner':\n",
    "        sign = '<#>'\n",
    "    select_query = f\"SELECT Id FROM {table_name2} where DocId = '{doc_id}' ORDER BY embedding <-> %s LIMIT {retrieve_k}\"\n",
    "    cursor.execute(select_query, (np.array(questionEmbedding),))\n",
    "    results = cursor.fetchall()\n",
    "    top_ids = []\n",
    "    for i in range(len(results)):\n",
    "        top_ids.append(int(results[i][0]))\n",
    "\n",
    "    if verbose:\n",
    "        print('top_ids:', top_ids)\n",
    "    # Rollback the current transaction\n",
    "    connection.rollback()\n",
    "\n",
    "    format_ids = ', '.join(['%s'] * len(top_ids))\n",
    "\n",
    "    sql = f\"SELECT Chunk FROM {table_name2} WHERE id IN ({format_ids})\"\n",
    "\n",
    "    # Execute the SELECT statement\n",
    "    try:\n",
    "        cursor.execute(sql, top_ids)    \n",
    "        top_rows = cursor.fetchall()\n",
    "    except (Exception, Error) as e:\n",
    "        print(f\"Error executing SELECT statement: {e}\")\n",
    "    finally:\n",
    "        pass\n",
    "    \n",
    "    sql_lines = f\"SELECT LineNumber FROM {table_name2} WHERE id IN ({format_ids})\"\n",
    "    # Execute the SELECT statement\n",
    "    try:\n",
    "        cursor.execute(sql_lines, top_ids)    \n",
    "        lines = cursor.fetchall()\n",
    "    except (Exception, Error) as e:\n",
    "        print(f\"Error executing SELECT statement: {e}\")\n",
    "    finally:\n",
    "        pass\n",
    "        # cursor.close()\n",
    "    #print(\"top_rows\", top_rows)\n",
    "    # getting teh \n",
    "    sql_pages = f\"SELECT PageNumber FROM {table_name2} WHERE id IN ({format_ids})\"\n",
    "    # Execute the SELECT statement\n",
    "    try:\n",
    "        cursor.execute(sql_pages, top_ids)    \n",
    "        pages = cursor.fetchall()\n",
    "    except (Exception, Error) as e:\n",
    "        print(f\"Error executing SELECT statement: {e}\")\n",
    "    finally:\n",
    "        cursor.close()    \n",
    "    \n",
    "    retrieved_pages = [int(page[0]) for page in pages]\n",
    "    retrieved_lines = [int(line[0]) for line in lines]\n",
    "    return top_rows, retrieved_pages, retrieved_lines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def create_json(pages, lines, rows):\n",
    "    if len(pages) != len(lines) or len(pages) != len(rows):\n",
    "        raise ValueError(\"The number of pages, lines, and rows must be the same.\")\n",
    "\n",
    "    data = {}\n",
    "    for page, line, row in zip(pages, lines, rows):\n",
    "        key = f\"{page}, {line}\"\n",
    "        data[key] = [row]\n",
    "\n",
    "    json_data = json.dumps(data)\n",
    "    return json_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def parse_json(text_with_json):\n",
    "    '''\n",
    "        # Sample JSON text (replace this with your actual JSON text)\n",
    "    json_text = \"\"\"\n",
    "    {\n",
    "        \"PageNumber\": \"2\",\n",
    "        \"LineNumber\": \"11\",\n",
    "        \"Answer\": \"<Enter Ground Truth Answer Here>\"\n",
    "    }\n",
    "    '''\n",
    "\n",
    "    # Step 1: Extract the JSON part from the larger text\n",
    "    start_index = text_with_json.find(\"{\")\n",
    "    end_index = text_with_json.rfind(\"}\")\n",
    "    if start_index == -1 or end_index == -1:\n",
    "        print(\"Error: JSON data not found in the text.\")\n",
    "        exit()\n",
    "\n",
    "    json_text = text_with_json[start_index:end_index+1]\n",
    "\n",
    "    # Step 2: Convert the JSON text to a Python dictionary\n",
    "    try:\n",
    "        parsed_data = json.loads(json_text)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error: The extracted text does not contain valid JSON data.\")\n",
    "        exit()\n",
    "\n",
    "    \n",
    "    # Step 3: Access the \"PageNumber\" field\n",
    "    if \"PageNumber\" in parsed_data:\n",
    "        page_number = parsed_data[\"PageNumber\"]\n",
    "        print(\"PageNumber:\", page_number)\n",
    "    else:\n",
    "        print(\"Error: The 'PageNumber' field was not found in the JSON data.\")\n",
    "        \n",
    "    if \"LineNumber\" in parsed_data:\n",
    "        line_number = parsed_data[\"LineNumber\"]\n",
    "        print(\"LineNumber:\", line_number)\n",
    "    \n",
    "    if \"Answer\" in parsed_data:\n",
    "        answer = parsed_data[\"Answer\"]\n",
    "        print(\"Answer:\", answer)\n",
    "    \n",
    "    \n",
    "    return page_number, line_number, answer\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.metadata import packages_distributions\n",
    "\n",
    "\n",
    "def get_user_questions_answers(retrieve_k, filter_id1_vals,similarity_method, QUESTION_PROMPT, verbose=False):\n",
    "    \"\"\"\n",
    "    Collection of user questions with known answers.\n",
    "    \"\"\"\n",
    "    Q = []\n",
    "    A = []\n",
    "    Agpt = []\n",
    "    contexts = []\n",
    "    ret_lines = []\n",
    "    ret_pages = []\n",
    "    pages = []\n",
    "    lines =[]\n",
    "    i = 0\n",
    "    for key, value in userQuestions.items():\n",
    "        if \"q\" in key:\n",
    "            Q.append(value)\n",
    "            questionEmbedding = createEmbeddings(value)\n",
    "            if verbose:\n",
    "                print(\"question: \", value)\n",
    "            top_rows, retreived_pages, retreived_lines = retrieve_k_chunk(retrieve_k, questionEmbedding,filter_id1_vals[i],similarity_method, verbose = verbose)\n",
    "            context = create_json(retreived_pages, retreived_lines, top_rows)\n",
    "            # create the context from the top_rows\n",
    "            # context = \"\"\n",
    "            # for row in top_rows:\n",
    "            #     context += row[0]\n",
    "            #     context += \"\\n\"\n",
    "            if verbose:\n",
    "                print('context: \\n', context)\n",
    "            loader = TextFormatter(context)\n",
    "            chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=QUESTION_PROMPT)\n",
    "            ans = chain({\"input_documents\": loader.load(), \"question\": value}, return_only_outputs=True)\n",
    "            #if verbose:\n",
    "            print('gpt_output', ans['output_text'])\n",
    "            PageNumber, LineNumber, Answer = parse_json(ans['output_text'])\n",
    "\n",
    "            Agpt.append(Answer)\n",
    "            contexts.append(context)\n",
    "            pages.append(int(PageNumber))\n",
    "            lines.append(int(LineNumber))\n",
    "            ret_pages.append(retreived_pages)\n",
    "            ret_lines.append(retreived_lines)\n",
    "            if verbose:\n",
    "                print(ans['output_text'])\n",
    "            i+=2\n",
    "        else:\n",
    "            A.append(value)\n",
    "        \n",
    "        \n",
    "    return  Q, A, Agpt, contexts, pages, lines, ret_pages, ret_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_text(config_dict):\n",
    "    config_text = \"\"\n",
    "    for key, value in config_dict.items():\n",
    "        config_text += f\"{key}: {value}\\n\"\n",
    "    return config_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(ExperimentConfig, QUESTION_PROMPT, verbose=False):\n",
    "    config_text = dict_to_text(ExperimentConfig)\n",
    "    Q, A, Agpt, contexts, page_numbers, line_numbers, ret_pages, ret_lines = get_user_questions_answers(retrieve_k = ExperimentConfig['retrieve_k'] ,filter_id1_vals=filter_id1_vals, similarity_method = ExperimentConfig['similarity_method'], QUESTION_PROMPT=QUESTION_PROMPT, verbose=verbose)\n",
    "    print(page_numbers)\n",
    "    QAres = [A, Agpt, Q]\n",
    "    cos_sim_scores = get_cosine_similarities(QAres, verbose= True)\n",
    "    #page_numbers, page_number_score = get_all_page_numbers_from_json(Agpt)\n",
    "    \n",
    "    page_number_score = [1 if page_numbers[i] == df_eval[\"PageNumber\"].tolist()[i] else 0 for i in range(len(page_numbers))]\n",
    "    df_evaluation = pd.DataFrame({'Question': Q, 'Answer': A, 'Answer_gpt': Agpt, 'Score': cos_sim_scores, 'detected_page_number': page_numbers, 'actual_page_number': df_eval[\"PageNumber\"].tolist(), 'page_number_score': page_number_score, 'context': contexts, \\\n",
    "        'retrieved_pages': ret_pages})\n",
    "    df_evaluation[\"correct_page_in_retrieved\"] = df_evaluation.apply(lambda row: row['actual_page_number'] in row['retrieved_pages'], axis=1)\n",
    "    df_evaluation[\"config\"] = config_text\n",
    "    df_evaluation[\"prompt\"] = QUESTION_PROMPT.template\n",
    "    current_timestamp = pd.Timestamp.now()\n",
    "    timestamp_str = current_timestamp.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    file_path ='..\\DATA\\evaluation'+ '_retrieve_'+ str(ExperimentConfig['retrieve_k']) + '_similarity_'+ ExperimentConfig['similarity_method']+'_date_' +timestamp_str + '_.csv'  \n",
    "    df_evaluation.to_csv(file_path, index=False)\n",
    "    return np.mean(cos_sim_scores), np.mean(page_number_score),  cos_sim_scores, page_number_score, df_evaluation, file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation using MLFLOW library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we would like to experiment with input and out format. We will pass input as json file and extract output as json file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.set_experiment(experiment_name=\"RAG_EXP\")\n",
    "    \n",
    "#import yaml\n",
    "#config = yaml.load(open(\"EvalConfig.yaml\", \"r\"), Loader=yaml.FullLoader)\n",
    "table_name1 = filter_id1_name\n",
    "table_name2 = 'ChunksEmbedding'\n",
    "ExperimentConfig1 = {'retrieve_k': 5, 'similarity_method': 'knn'} # similarity = ['cosine', 'NN', 'inner']\n",
    "\n",
    "\n",
    "RUNConfigs = [ExperimentConfig1]\n",
    "\n",
    "question_prompt_template = \"\"\"Use the following portion of the context document to find relevant text and answer the question in details. Extract PageNumber and LineNumber and show it in the answer. \n",
    "    {context}\n",
    "    Question: {question}\n",
    "    If the answer is not found, please double check as it is most likely in the provided context, and only if you are sure say that answer is not available in the documentation, in this case the value for pagenumber and line number would be NA.\n",
    "    If you found an answer from the provided text, make sure to provide the answer in a json format with PageNumber, LineNumber, and answer as the json keys. \n",
    "    \n",
    "    \"\"\"\n",
    "QUESTION_PROMPT = PromptTemplate(\n",
    "        template=question_prompt_template, input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "for ExperimentConfig in RUNConfigs:\n",
    "    current_timestamp = pd.Timestamp.now()\n",
    "    timestamp_str = current_timestamp.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    mlflow_run_name = \"Retreive_k_\" + str(ExperimentConfig['retrieve_k']) + \"_similarity_\" + ExperimentConfig['similarity_method']+ \"_date_\" + timestamp_str\n",
    "    with mlflow.start_run(run_name=mlflow_run_name) as run:\n",
    "        mean_sim_score, mean_page_score,cos_sim_score, page_number_score, df_evaluation, df_path = run_experiment(ExperimentConfig, QUESTION_PROMPT, verbose = False)\n",
    "        print(\"mean_sim_score, mean_page_score:\", mean_sim_score, mean_page_score)\n",
    "        mlflow.log_metric(\"mean_sim_score\", mean_sim_score)\n",
    "        mlflow.log_metric(\"mean_page_score\", mean_page_score)\n",
    "        mlflow.log_param(\"cosine_similarity_score\", str(cos_sim_score))\n",
    "        mlflow.log_param(\"page_number_score\", str(page_number_score))\n",
    "        mlflow.log_param(\"Client code table\", table_name1)\n",
    "        mlflow.log_param(\"Chunk table\", table_name2)\n",
    "        #mlflow.log_param(\"prompt\", QUESTION_PROMPT)  \n",
    "        mlflow.log_param(\"config\", \"\".join(ExperimentConfig))\n",
    "        mlflow.log_artifact(df_path)\n",
    "        print(\"config :\\n\", ExperimentConfig)\n",
    "mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flexenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
