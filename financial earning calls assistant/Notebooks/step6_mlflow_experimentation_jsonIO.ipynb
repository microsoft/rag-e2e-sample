{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON IO: In this notebook, we will experiment with prompt template. We will pass input context as json and extract output answer from gpt as json format as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load configs for database, Azure OpenAI, and other resources as environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "\n",
    "# specify the name of the .env file name \n",
    "env_name = \"llm_pgvector.env\" # change to your own .env file name\n",
    "config = dotenv_values(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Flex Postgres (PG)  for retrievals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection pool created successfully\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import pool\n",
    "from psycopg2 import Error\n",
    "\n",
    "host = config[\"HOST\"]\n",
    "dbname = config[\"DBNAME\"] \n",
    "user = config[\"USER\"] \n",
    "password = config[\"PASSWORD\"] \n",
    "sslmode = config[\"SSLMODE\"]  \n",
    "\n",
    "# Build a connection string from the variables\n",
    "conn_string = \"host={0} user={1} dbname={2} password={3} sslmode={4}\".format(host, user, dbname, password, sslmode)\n",
    "\n",
    "postgreSQL_pool = psycopg2.pool.SimpleConnectionPool(1, 20,conn_string)\n",
    "if (postgreSQL_pool):\n",
    "    print(\"Connection pool created successfully\")\n",
    "\n",
    "# Use getconn() to get a connection from the connection pool\n",
    "connection = postgreSQL_pool.getconn()\n",
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare for question embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "openai.api_type = config[\"OPENAI_API_TYPE\"] \n",
    "openai.api_key = config['OPENAI_API_KEY']\n",
    "openai.api_base = config['OPENAI_API_BASE'] \n",
    "openai.api_version = config['OPENAI_API_VERSION']  \n",
    "\n",
    "\n",
    "def createEmbeddings(text):\n",
    "    response = openai.Embedding.create(input=text , engine=config[\"OPENAI_DEPLOYMENT_EMBEDDING\"])\n",
    "    embeddings = response['data'][0]['embedding']\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define llm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai.api_key\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://synapseml-openai.openai.azure.com/\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2022-12-01\"\n",
    "os.environ[\"OPENAI_DEPLOYMENT_NAME\"] = \"text-davinci-003\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import AzureOpenAI\n",
    "llm= AzureOpenAI(deployment_name=config[\"OPENAI_MODEL_COMPLETION\"], model_name=config[\"OPENAI_MODEL_EMBEDDING\"], temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders.base import BaseLoader\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "class TextFormatter(BaseLoader):\n",
    "    \"\"\"Load text files.\"\"\"\n",
    "\n",
    "    def __init__(self, text: str):\n",
    "        \"\"\"Initialize with file path.\"\"\"\n",
    "        self.text = text\n",
    "\n",
    "    def load(self) -> List[Document]:\n",
    "        \"\"\"Load from file path.\"\"\"\n",
    "        metadata = {\"source\": \"\"}\n",
    "        return [Document(page_content=self.text, metadata=metadata)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Navigate to the directory containing the CSV file (one level above the current directory)\n",
    "data_directory = os.path.abspath(os.path.join(current_directory, '..', 'ValidationSetOfQA'))\n",
    "\n",
    "# Construct the file path for your CSV file in the data_directory\n",
    "csv_file_path = os.path.join(data_directory, 'QnAValidationSet.csv')\n",
    "\n",
    "# Load the CSV file using pandas\n",
    "df = pd.read_csv(csv_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Ticker', 'Quarter', 'Question', 'Answer', 'PageNumber'], dtype='object')"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the filters you want to apply \n",
    "filter1_name = 'Ticker'\n",
    "filter2_name = 'Quarter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Quarter</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>PageNumber</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>1</td>\n",
       "      <td>How did the first quarter financial results co...</td>\n",
       "      <td>In the first quarter, our revenue reached $50....</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>1</td>\n",
       "      <td>What were the key highlights and growth trends...</td>\n",
       "      <td>Within the commercial business segment, we wit...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Ticker  Quarter                                           Question  \\\n",
       "0   MSFT        1  How did the first quarter financial results co...   \n",
       "1   MSFT        1  What were the key highlights and growth trends...   \n",
       "\n",
       "                                              Answer  PageNumber  \n",
       "0  In the first quarter, our revenue reached $50....          18  \n",
       "1  Within the commercial business segment, we wit...          19  "
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval = df.copy()\n",
    "df_eval.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [item for pair in zip(list(df_eval['Question']), list(df_eval['Answer'])) for item in pair]\n",
    "keys = [str(i//2)+'a' if i%2==0 else str(i//2+1)+'q' for i in range(1,len(values)+2)]\n",
    "\n",
    "userQuestions = {keys[i]:values[i] for i in range(len(keys)-1)}\n",
    "filter1_vals = [item for item in list(df_eval[filter1_name]) for _ in range(2)]\n",
    "filter2_vals = [item for item in list(df_eval[filter2_name]) for _ in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT based question answering with type checking\n",
    "from langchain import PromptTemplate\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_similarities(QA_results, verbose = False):\n",
    "    # compare cosine similarity between two vectors\n",
    "    cosine_similarities = []\n",
    "    for i in range(len(QA_results[0])):\n",
    "        if verbose:\n",
    "            print('calculating cosine similarity for: \\n', QA_results[0][i], '\\n', QA_results[1][i])\n",
    "            print(30*'-')\n",
    "        emd1 = createEmbeddings(QA_results[0][i])\n",
    "        emd2 = createEmbeddings(QA_results[1][i])\n",
    "        cosine_similarity_val = cosine_similarity(\n",
    "            np.array(emd1).reshape(1, -1), np.array(emd2).reshape(1, -1)\n",
    "        )[0][0]\n",
    "        cosine_similarities.append(np.round(cosine_similarity_val, 2))\n",
    "    return cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's also evaluate the reference page number\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_page_number(text):\n",
    "    # Regular expression pattern to find the PageNumber value\n",
    "    pattern = r'PageNumber:\\s+(\\d+)'\n",
    "\n",
    "    # Search for the pattern in the text\n",
    "    match = re.search(pattern, text)\n",
    "\n",
    "    # If a match is found, return the extracted PageNumber value, otherwise return 0\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_all_page_numbers(Agpt):\n",
    "    page_numbers = []\n",
    "    for answers in Agpt:\n",
    "        page_numbers.append(extract_page_number(answers))\n",
    "    print(page_numbers)\n",
    "    print(df_eval[\"PageNumber\"].tolist())\n",
    "    page_number_score = [1 if page_numbers[i] == df_eval[\"PageNumber\"].tolist()[i] else 0 for i in range(len(page_numbers))]\n",
    "    print(page_number_score)\n",
    "    return page_numbers, page_number_score\n",
    "    \n",
    "    \n",
    "def get_all_page_numbers_from_json(Agpt):\n",
    "    page_numbers = []\n",
    "    for answers in Agpt:\n",
    "        print('page numbers: ', answers[\"Answer\"][\"PageNumber\"])\n",
    "        print(type(answers))\n",
    "        print(type(answers[\"Answer\"][\"PageNumber\"]))\n",
    "        #page_numbers.append(int(answers[\"Answer\"][\"PageNumber\"]))\n",
    "    print(page_numbers)\n",
    "    print(df_eval[\"PageNumber\"].tolist())\n",
    "    page_number_score = [1 if page_numbers[i] == df_eval[\"PageNumber\"].tolist()[i] else 0 for i in range(len(page_numbers))]\n",
    "    print(page_number_score)\n",
    "    return page_numbers, page_number_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgvector.psycopg2 import register_vector\n",
    "\n",
    "def retrieve_k_chunk(retrieve_k, questionEmbedding, filter1_val, filter2_val, similarity_method=\"knn\", verbose=False):\n",
    "    connection = psycopg2.connect(conn_string)\n",
    "    # Create a cursor after the connection\n",
    "    # Register 'pgvector' type for the 'embedding' column\n",
    "    register_vector(connection)\n",
    "    cursor = connection.cursor()\n",
    "    \n",
    " \n",
    "    if similarity_method == 'knn':\n",
    "        sign = '<->'\n",
    "    elif similarity_method=='cosine':\n",
    "        sign = '<=>'\n",
    "    elif similarity_method=='inner':\n",
    "        sign = '<#>'\n",
    "    select_query = f\"SELECT Id FROM {table_name} where {filter1_name} = '{filter1_val}' and {filter2_name}='{filter2_val}' ORDER BY embedding {sign} %s LIMIT {retrieve_k}\"\n",
    "    cursor.execute(select_query, (np.array(questionEmbedding),))\n",
    "    results = cursor.fetchall()\n",
    "    top_ids = []\n",
    "    for i in range(len(results)):\n",
    "        top_ids.append(int(results[i][0]))\n",
    "\n",
    "    if verbose:\n",
    "        print('top_ids:', top_ids)\n",
    "    # Rollback the current transaction\n",
    "    connection.rollback()\n",
    "\n",
    "    format_ids = ', '.join(['%s'] * len(top_ids))\n",
    "\n",
    "    sql = f\"SELECT CONCAT('PageNumber: ', PageNumber, ' ', 'LineNumber: ', LineNumber, ' ', 'Text: ', Chunk) AS concat FROM {table_name} WHERE id IN ({format_ids})\"\n",
    "\n",
    "    # Execute the SELECT statement\n",
    "    try:\n",
    "        cursor.execute(sql, top_ids)    \n",
    "        top_rows = cursor.fetchall()\n",
    "    except (Exception, Error) as e:\n",
    "        print(f\"Error executing SELECT statement: {e}\")\n",
    "    finally:\n",
    "        pass\n",
    "    \n",
    "    sql_lines = f\"SELECT LineNumber FROM {table_name} WHERE id IN ({format_ids})\"\n",
    "    # Execute the SELECT statement\n",
    "    try:\n",
    "        cursor.execute(sql_lines, top_ids)    \n",
    "        lines = cursor.fetchall()\n",
    "    except (Exception, Error) as e:\n",
    "        print(f\"Error executing SELECT statement: {e}\")\n",
    "    finally:\n",
    "        pass\n",
    "        # cursor.close()\n",
    "    #print(\"top_rows\", top_rows)\n",
    "    # getting teh \n",
    "    sql_pages = f\"SELECT PageNumber FROM {table_name} WHERE id IN ({format_ids})\"\n",
    "    # Execute the SELECT statement\n",
    "    try:\n",
    "        cursor.execute(sql_pages, top_ids)    \n",
    "        pages = cursor.fetchall()\n",
    "    except (Exception, Error) as e:\n",
    "        print(f\"Error executing SELECT statement: {e}\")\n",
    "    finally:\n",
    "        cursor.close()    \n",
    "    \n",
    "    retrieved_pages = [int(page[0]) for page in pages]\n",
    "    retrieved_lines = [int(line[0]) for line in lines]\n",
    "    return top_rows, retrieved_pages, retrieved_lines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def create_json(pages, lines, rows):\n",
    "    if len(pages) != len(lines) or len(pages) != len(rows):\n",
    "        raise ValueError(\"The number of pages, lines, and rows must be the same.\")\n",
    "\n",
    "    data = {}\n",
    "    for page, line, row in zip(pages, lines, rows):\n",
    "        key = f\"{page}, {line}\"\n",
    "        data[key] = [row]\n",
    "\n",
    "    json_data = json.dumps(data)\n",
    "    return json_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use this section only if you wish to format the json output (commented). Otherwise use the next cell for pydantic approach (recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# def parse_json(text_with_json):\n",
    "#     '''\n",
    "#         # Sample JSON text (replace this with your actual JSON text)\n",
    "#     json_text = \"\"\"\n",
    "#     {\n",
    "#         \"PageNumber\": \"2\",\n",
    "#         \"LineNumber\": \"11\",\n",
    "#         \"Answer\": \"<Enter Ground Truth Answer Here>\"\n",
    "#     }\n",
    "#     '''\n",
    "\n",
    "#     # Step 1: Extract the JSON part from the larger text\n",
    "#     start_index = text_with_json.find(\"{\")\n",
    "#     end_index = text_with_json.rfind(\"}\")\n",
    "#     if start_index == -1 or end_index == -1:\n",
    "#         print(\"Error: JSON data not found in the text.\")\n",
    "#         exit()\n",
    "\n",
    "#     json_text = text_with_json[start_index:end_index+1]\n",
    "\n",
    "#     # Step 2: Convert the JSON text to a Python dictionary\n",
    "#     try:\n",
    "#         parsed_data = json.loads(json_text)\n",
    "#     except json.JSONDecodeError:\n",
    "#         print(\"Error: The extracted text does not contain valid JSON data.\")\n",
    "#         exit()\n",
    "\n",
    "    \n",
    "#     # Step 3: Access the \"PageNumber\" field\n",
    "#     if \"PageNumber\" in parsed_data:\n",
    "#         page_number = parsed_data[\"PageNumber\"]\n",
    "#         print(\"PageNumber:\", page_number)\n",
    "#     else:\n",
    "#         print(\"Error: The 'PageNumber' field was not found in the JSON data.\")\n",
    "        \n",
    "#     if \"LineNumber\" in parsed_data:\n",
    "#         line_number = parsed_data[\"LineNumber\"]\n",
    "#         print(\"LineNumber:\", line_number)\n",
    "    \n",
    "#     if \"Answer\" in parsed_data:\n",
    "#         answer = parsed_data[\"Answer\"]\n",
    "#         print(\"Answer:\", answer)\n",
    "    \n",
    "    \n",
    "#     return page_number, line_number, answer\n",
    "\n",
    "# def get_user_questions_answers(retrieve_k, filter1_vals, filter2_vals, similarity_method, QUESTION_PROMPT, verbose=False):\n",
    "#     \"\"\"\n",
    "#     Collection of user questions with known answers.\n",
    "#     \"\"\"\n",
    "#     Q = []\n",
    "#     A = []\n",
    "#     Agpt = []\n",
    "#     contexts = []\n",
    "#     ret_lines = []\n",
    "#     ret_pages = []\n",
    "#     pages = []\n",
    "#     lines =[]\n",
    "#     i = 0\n",
    "#     for key, value in userQuestions.items():\n",
    "#         if \"q\" in key:\n",
    "#             Q.append(value)\n",
    "#             questionEmbedding = createEmbeddings(value)\n",
    "#             if verbose:\n",
    "#                 print(\"question: \", value)\n",
    "#             top_rows, retreived_pages, retreived_lines = retrieve_k_chunk(retrieve_k, questionEmbedding,filter1_vals[i],filter_id2_vals[i],similarity_method, verbose = verbose)\n",
    "#             context = create_json(retreived_pages, retreived_lines, top_rows)\n",
    "#             # create the context from the top_rows\n",
    "#             # context = \"\"\n",
    "#             # for row in top_rows:\n",
    "#             #     context += row[0]\n",
    "#             #     context += \"\\n\"\n",
    "#             if verbose:\n",
    "#                 print('context: \\n', context)\n",
    "#             loader = TextFormatter(context)\n",
    "#             chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=QUESTION_PROMPT)\n",
    "#             ans = chain({\"input_documents\": loader.load(), \"question\": value}, return_only_outputs=True)\n",
    "#             #if verbose:\n",
    "#             print('gpt_output', ans['output_text'])\n",
    "#             PageNumber, LineNumber, Answer = parse_json(ans['output_text'])\n",
    "\n",
    "#             Agpt.append(Answer)\n",
    "#             contexts.append(context)\n",
    "#             pages.append(int(PageNumber))\n",
    "#             lines.append(int(LineNumber))\n",
    "#             ret_pages.append(retreived_pages)\n",
    "#             ret_lines.append(retreived_lines)\n",
    "#             if verbose:\n",
    "#                 print(ans['output_text'])\n",
    "#             i+=2\n",
    "#         else:\n",
    "#             A.append(value)\n",
    "        \n",
    "        \n",
    "#     return  Q, A, Agpt, contexts, pages, lines, ret_pages, ret_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the following cell to extract json output using pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List\n",
    "# Here's another example, but with a compound typed field.\n",
    "class LLMAnswer(BaseModel):\n",
    "    PageNumber: int = Field(description=\"reference page number\")\n",
    "    LineNumber: int = Field(description=\"reference line number\")\n",
    "    Answer: str= Field(description = \"answer found in the text\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=LLMAnswer)\n",
    "\n",
    "def get_user_questions_answers(retrieve_k, filter1_vals, filter2_vals, similarity_method, QUESTION_PROMPT, verbose=False):\n",
    "    \"\"\"\n",
    "    Collection of user questions with known answers.\n",
    "    \"\"\"\n",
    "    Q = []\n",
    "    A = []\n",
    "    Agpt = []\n",
    "    contexts = []\n",
    "    ret_lines = []\n",
    "    ret_pages = []\n",
    "    pages = []\n",
    "    lines =[]\n",
    "    i = 0\n",
    "    for key, value in userQuestions.items():\n",
    "        if \"q\" in key:\n",
    "            Q.append(value)\n",
    "            questionEmbedding = createEmbeddings(value)\n",
    "            if verbose:\n",
    "                print(\"question: \", value)\n",
    "            top_rows, retreived_pages, retreived_lines = retrieve_k_chunk(retrieve_k, questionEmbedding,filter1_vals[i],filter2_vals[i],similarity_method, verbose = verbose)\n",
    "            context = create_json(retreived_pages, retreived_lines, top_rows)\n",
    "            # create the context from the top_rows\n",
    "            # context = \"\"\n",
    "            # for row in top_rows:\n",
    "            #     context += row[0]\n",
    "            #     context += \"\\n\"\n",
    "            if verbose:\n",
    "                print('context: \\n', context)\n",
    "            loader = TextFormatter(context)\n",
    "            chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=QUESTION_PROMPT)\n",
    "            ans = chain({\"input_documents\": loader.load(), \"question\": value}, return_only_outputs=True)\n",
    "            #if verbose:\n",
    "            print('gpt_output', ans['output_text'])\n",
    "            PageNumber, LineNumber, Answer = parse_json(ans['output_text'])\n",
    "\n",
    "            parsed_ans = parser.parse(ans['output_text'])\n",
    "            Agpt.append(parsed_ans.Answer)\n",
    "            contexts.append(context)\n",
    "            pages.append(parsed_ans.PageNumber)\n",
    "            lines.append(parsed_ans.LineNumber)\n",
    "            ret_pages.append(retreived_pages)\n",
    "            ret_lines.append(retreived_lines)\n",
    "\n",
    "            if verbose:\n",
    "                print(ans['output_text'])\n",
    "            i+=2\n",
    "        else:\n",
    "            A.append(value)\n",
    "        \n",
    "    return  Q, A, Agpt, contexts, pages, lines, ret_pages, ret_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_text(config_dict):\n",
    "    config_text = \"\"\n",
    "    for key, value in config_dict.items():\n",
    "        config_text += f\"{key}: {value}\\n\"\n",
    "    return config_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(ExperimentConfig, QUESTION_PROMPT, verbose=False):\n",
    "    config_text = dict_to_text(ExperimentConfig)\n",
    "    Q, A, Agpt, contexts, page_numbers, line_numbers, ret_pages, ret_lines = get_user_questions_answers(retrieve_k = ExperimentConfig['retrieve_k'] ,filter1_vals = filter1_vals, filter2_vals = filter2_vals, similarity_method = ExperimentConfig['similarity_method'], QUESTION_PROMPT=QUESTION_PROMPT, verbose=verbose)\n",
    "    print(page_numbers)\n",
    "    QAres = [A, Agpt, Q]\n",
    "    cos_sim_scores = get_cosine_similarities(QAres, verbose= True)\n",
    "    #page_numbers, page_number_score = get_all_page_numbers_from_json(Agpt)\n",
    "    \n",
    "    page_number_score = [1 if page_numbers[i] == df_eval[\"PageNumber\"].tolist()[i] else 0 for i in range(len(page_numbers))]\n",
    "    df_evaluation = pd.DataFrame({'Question': Q, 'Answer': A, 'Answer_gpt': Agpt, 'Score': cos_sim_scores, 'detected_page_number': page_numbers, 'actual_page_number': df_eval[\"PageNumber\"].tolist(), 'page_number_score': page_number_score, 'context': contexts, \\\n",
    "        'retrieved_pages': ret_pages})\n",
    "    df_evaluation[\"correct_page_in_retrieved\"] = df_evaluation.apply(lambda row: row['actual_page_number'] in row['retrieved_pages'], axis=1)\n",
    "    df_evaluation[\"config\"] = config_text\n",
    "    df_evaluation[\"prompt\"] = QUESTION_PROMPT.template\n",
    "    current_timestamp = pd.Timestamp.now()\n",
    "    timestamp_str = current_timestamp.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    file_path ='..\\DATA\\evaluation'+ '_retrieve_'+ str(ExperimentConfig['retrieve_k']) + '_similarity_'+ ExperimentConfig['similarity_method']+'_date_' +timestamp_str + '_.csv'  \n",
    "    df_evaluation.to_csv(file_path, index=False)\n",
    "    return np.mean(cos_sim_scores), np.mean(page_number_score),  cos_sim_scores, page_number_score, df_evaluation, file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation using MLFLOW library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we would like to experiment with input and out format. We will pass input as json file and extract output as json file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt_output \n",
      "Answer: {\n",
      "    \"PageNumber\": 1,\n",
      "    \"LineNumber\": 17,\n",
      "    \"Answer\": \"The first quarter revenue was $50.1 billion, up 11 percent and 16 percent in constant currency. Earnings per share was $2.35 - and increased 4 percent and 11 percent in constant currency, when adjusted for the net tax benefit from the first quarter of fiscal year 22.\"\n",
      "}\n",
      "PageNumber: 1\n",
      "LineNumber: 17\n",
      "Answer: The first quarter revenue was $50.1 billion, up 11 percent and 16 percent in constant currency. Earnings per share was $2.35 - and increased 4 percent and 11 percent in constant currency, when adjusted for the net tax benefit from the first quarter of fiscal year 22.\n",
      "gpt_output \n",
      "Answer: {\n",
      "    \"PageNumber\": 19, \n",
      "    \"LineNumber\": 9, \n",
      "    \"Answer\": \"In the commercial business segment, there was strong overall demand for Microsoft Cloud offerings, with growth of 31 percent in constant currency as well as share gains across many businesses. Commercial bookings declined 3 percent and increased 16 percent in constant currency on a flat expiry base. Excluding the FX impact, growth was driven by strong renewal execution and there was growth in the number of large, long-term Azure and Microsoft 365 contracts across all deal sizes.\"\n",
      "}\n",
      "PageNumber: 19\n",
      "LineNumber: 9\n",
      "Answer: In the commercial business segment, there was strong overall demand for Microsoft Cloud offerings, with growth of 31 percent in constant currency as well as share gains across many businesses. Commercial bookings declined 3 percent and increased 16 percent in constant currency on a flat expiry base. Excluding the FX impact, growth was driven by strong renewal execution and there was growth in the number of large, long-term Azure and Microsoft 365 contracts across all deal sizes.\n",
      "[1, 19]\n",
      "calculating cosine similarity for: \n",
      " In the first quarter, our revenue reached $50.1 billion, representing an 11 percent increase or 16 percent growth when adjusted for constant currency. Earnings per share stood at $2.35, showing a 4 percent increase or 11 percent growth in constant currency, after accounting for the net tax benefit from the previous year's first quarter. \n",
      " The first quarter revenue was $50.1 billion, up 11 percent and 16 percent in constant currency. Earnings per share was $2.35 - and increased 4 percent and 11 percent in constant currency, when adjusted for the net tax benefit from the first quarter of fiscal year 22.\n",
      "------------------------------\n",
      "calculating cosine similarity for: \n",
      " Within the commercial business segment, we witnessed strong demand for our Microsoft Cloud offerings, resulting in a remarkable 31 percent growth in constant currency. Moreover, we achieved share gains across multiple businesses. While commercial bookings experienced a 3 percent decline, they increased by 16 percent in constant currency on a flat expiry base. This growth was driven by robust renewal execution and an increase in large, long-term contracts for Azure and Microsoft 365 across various deal sizes. Notably, more than half of the bookings exceeding $10 million in value came from Microsoft 365 E5. \n",
      " In the commercial business segment, there was strong overall demand for Microsoft Cloud offerings, with growth of 31 percent in constant currency as well as share gains across many businesses. Commercial bookings declined 3 percent and increased 16 percent in constant currency on a flat expiry base. Excluding the FX impact, growth was driven by strong renewal execution and there was growth in the number of large, long-term Azure and Microsoft 365 contracts across all deal sizes.\n",
      "------------------------------\n",
      "mean_sim_score, mean_page_score: 0.99 0.5\n",
      "config :\n",
      " {'retrieve_k': 5, 'similarity_method': 'knn'}\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "mlflow.set_experiment(experiment_name=\"RAG_EXP\")\n",
    "    \n",
    "#import yaml\n",
    "#config = yaml.load(open(\"EvalConfig.yaml\", \"r\"), Loader=yaml.FullLoader)\n",
    "table_name = 'EarningsCallChunksEmbedding'\n",
    "\n",
    "ExperimentConfig1 = {'retrieve_k': 5, 'similarity_method': 'knn'} # similarity = ['cosine', 'NN', 'inner']\n",
    "\n",
    "RUNConfigs = [ExperimentConfig1]\n",
    "\n",
    "question_prompt_template = \"\"\"Use the following portion of the context document to find relevant text and answer the question in details. Extract PageNumber and LineNumber and show it in the answer. \n",
    "    {context}\n",
    "    Question: {question}\n",
    "    If the answer is not found, please double check as it is most likely in the provided context, and only if you are sure say that answer is not available in the documentation, in this case the value for pagenumber and line number would be NA.\n",
    "    If you found an answer from the provided text, make sure to provide the answer in a json format with PageNumber, LineNumber, and answer as the json keys. \n",
    "    \n",
    "    \"\"\"\n",
    "QUESTION_PROMPT = PromptTemplate(\n",
    "        template=question_prompt_template, input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "for ExperimentConfig in RUNConfigs:\n",
    "    current_timestamp = pd.Timestamp.now()\n",
    "    timestamp_str = current_timestamp.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    mlflow_run_name = \"Retreive_k_\" + str(ExperimentConfig['retrieve_k']) + \"_similarity_\" + ExperimentConfig['similarity_method']+ \"_date_\" + timestamp_str\n",
    "    with mlflow.start_run(run_name=mlflow_run_name) as run:\n",
    "        mean_sim_score, mean_page_score,cos_sim_score, page_number_score, df_evaluation, df_path = run_experiment(ExperimentConfig, QUESTION_PROMPT, verbose = False)\n",
    "        print(\"mean_sim_score, mean_page_score:\", mean_sim_score, mean_page_score)\n",
    "        mlflow.log_metric(\"mean_sim_score\", mean_sim_score)\n",
    "        mlflow.log_metric(\"mean_page_score\", mean_page_score)\n",
    "        mlflow.log_param(\"cosine_similarity_score\", str(cos_sim_score))\n",
    "        mlflow.log_param(\"page_number_score\", str(page_number_score))\n",
    "        mlflow.log_param(\"Client code table\", table_name)\n",
    "        #mlflow.log_param(\"prompt\", QUESTION_PROMPT)  \n",
    "        mlflow.log_param(\"config\", \"\".join(ExperimentConfig))\n",
    "        mlflow.log_artifact(df_path)\n",
    "        print(\"config :\\n\", ExperimentConfig)\n",
    "mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flexenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
