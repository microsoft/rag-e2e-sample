{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2469417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import openai\n",
    "from dotenv import dotenv_values\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "sys.path.append(\"..\") ## add directory above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1af107d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"../llm.env\" # change to use your own .env file\n",
    "config = dotenv_values(env_name)\n",
    "\n",
    "#Azure OpenAI\n",
    "openai.api_type = config[\"OPENAI_API_TYPE\"] #\"azure\"\n",
    "openai.api_key = config['OPENAI_API_KEY']\n",
    "openai.api_base = config['OPENAI_API_BASE']\n",
    "openai.api_version = config['OPENAI_API_VERSION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9c886c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = \"gpt-4-32k\"\n",
    "#engine = \"gpt-35-turbo\"\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=engine,\n",
    "    openai_api_base=openai.api_base,\n",
    "    openai_api_version=openai.api_version,\n",
    "    openai_api_key=openai.api_key,\n",
    "    openai_api_type = openai.api_type,\n",
    "    temperature=0.0, verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b0e6f8",
   "metadata": {},
   "source": [
    "## 1. Demonstrating conversation summary memory chain:\n",
    "This chain summarizes the previous user conversation and appends the summary to context for answering questions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4886138d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: \n",
      "USSA is a space agency in county Y. It is a government agency responsible for the exploration and development of space.\n",
      "Question 2: \n",
      "Yes, I mentioned earlier about the USSA, which is a government space agency responsible for space exploration and development in county Y.\n",
      "\n",
      "Context in question 2 does not contain any specific information regarding the \n",
      "       user question but still llm provides correct answer by using the memory of previous conversation\n",
      "       \n"
     ]
    }
   ],
   "source": [
    "from chatbotSkills import qa_chain_ConversationSummaryMemory\n",
    "\n",
    "# Make a Question Answer chain function and pass \n",
    "prefix_template = \"\"\"\n",
    "    You are a chatbot having a conversation with a human.\n",
    "    Given the Context, Chat History, and a Human Query, \n",
    "    create a final answer. Don't hallucinate at all. If you don't have an answer, say \"I don't know\".\n",
    "    \"\"\"\n",
    "\n",
    "qa_chain = qa_chain_ConversationSummaryMemory(llm, prefix_template=prefix_template, to_debug=False)\n",
    "\n",
    "## Question Answering\n",
    "\n",
    "#Question 1\n",
    "answer = qa_chain.run({\n",
    "    'context': \"USSA is a space agency in county Y. It is a government agency responsible for the exploration and development of space.\",\n",
    "    'human_input': \"What is USSA\" \n",
    "})\n",
    "\n",
    "print(\"Question 1: \")\n",
    "print(answer)\n",
    "\n",
    "# Question 2: \n",
    "answer = qa_chain.run({\n",
    "    'context': \"Zootopia is a 2016 American computer-animated buddy cop action comedy film produced by Walt Disney Animation Studios.\",\n",
    "    'human_input': \"Do you know about any space agency?\" \n",
    "}) \n",
    "\n",
    "print(\"Question 2: \")\n",
    "print(answer)\n",
    "print()\n",
    "print(\"\"\"Context in question 2 does not contain any specific information regarding the \n",
    "       user question but still llm provides correct answer by using the memory of previous conversation\n",
    "       \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb443051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human_prefix='Human' ai_prefix='AI' llm=AzureChatOpenAI(cache=None, verbose=True, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='48bb094bd3a24918b1be7d6de06d45ee', openai_api_base='https://journeyman-openai.openai.azure.com/', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None, deployment_name='gpt-4-32k', model_version='', openai_api_type='azure', openai_api_version='2023-05-15') prompt=PromptTemplate(input_variables=['summary', 'new_lines'], output_parser=None, partial_variables={}, template='\\n    Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\\n\\n    Current summary:\\n    {summary}\\n\\n    New lines of conversation:\\n    {new_lines}\\n\\n    New summary:', template_format='f-string', validate_template=True) summary_message_cls=<class 'langchain.schema.messages.SystemMessage'> chat_memory=ChatMessageHistory(messages=[HumanMessage(content='What is USSA', additional_kwargs={}, example=False), AIMessage(content='USSA is a space agency in county Y. It is a government agency responsible for the exploration and development of space.', additional_kwargs={}, example=False), HumanMessage(content='Do you know about any space agency?', additional_kwargs={}, example=False), AIMessage(content='Yes, I mentioned earlier about the USSA, a space agency located in county Y, responsible for space exploration and development.', additional_kwargs={}, example=False)]) output_key=None input_key='human_input' return_messages=False buffer='The human inquired about any space agencies, and the AI reiterated its previous information about the USSA, a space agency in county Y that is responsible for space exploration and development.' memory_key='chat_history'\n"
     ]
    }
   ],
   "source": [
    "print(qa_chain.memory) ## You can see the memory using this call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af592da",
   "metadata": {},
   "source": [
    "## 2. Demonstrating conversation buffer memory chain:\n",
    "This chain summarizes the previous user conversation and appends the summary to context for answering questions \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69704b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: \n",
      "USSA is a space agency in county Y. It is a government agency responsible for the exploration and development of space.\n",
      "Question 2: \n",
      "Yes, I do. One example is the USSA, a space agency in county Y. It is a government agency responsible for the exploration and development of space.\n",
      "\n",
      "Context in question 2 does not contain any specific information regarding the \n",
      "       user question but still llm provides correct answer by using the memory of previous conversation\n"
     ]
    }
   ],
   "source": [
    "from chatbotSkills import qa_chain_ConversationBufferMemory\n",
    "\n",
    "# Make a Question Answer chain function and pass \n",
    "prefix_template = \"\"\"\n",
    "    You are a chatbot having a conversation with a human. \n",
    "    Given the Context, Chat History, and a Human Query, \n",
    "    create a final answer. Don't hallucinate at all. If you don't have an answer, say \"I don't know\".\n",
    "    \"\"\"\n",
    "\n",
    "qa_chain = qa_chain_ConversationBufferMemory(llm, prefix_template=prefix_template, to_debug=False)\n",
    "\n",
    "## Question Answering\n",
    "\n",
    "#Question 1\n",
    "answer = qa_chain.run({\n",
    "   'context': \"USSA is a space agency in county Y. It is a government agency responsible for the exploration and development of space.\",\n",
    "   'human_input': \"What is USSA\" \n",
    "})\n",
    "\n",
    "print(\"Question 1: \")\n",
    "print(answer)\n",
    "\n",
    "# Question 2: \n",
    "answer = qa_chain.run({\n",
    "    'context': \"Zootopia is a 2016 American computer-animated buddy cop action comedy film produced by Walt Disney Animation Studios.\",\n",
    "    'human_input': \"Do you know about any space agency?\" \n",
    "}) \n",
    "\n",
    "print(\"Question 2: \")\n",
    "print(answer)\n",
    "print()\n",
    "print(\"\"\"Context in question 2 does not contain any specific information regarding the \n",
    "       user question but still llm provides correct answer by using the memory of previous conversation\n",
    "       \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb3356e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human_prefix='Human' ai_prefix='AI' llm=AzureChatOpenAI(cache=None, verbose=True, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='48bb094bd3a24918b1be7d6de06d45ee', openai_api_base='https://journeyman-openai.openai.azure.com/', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None, deployment_name='gpt-4-32k', model_version='', openai_api_type='azure', openai_api_version='2023-05-15') prompt=PromptTemplate(input_variables=['summary', 'new_lines'], output_parser=None, partial_variables={}, template='\\n    Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\\n\\n    Current summary:\\n    {summary}\\n\\n    New lines of conversation:\\n    {new_lines}\\n\\n    New summary:', template_format='f-string', validate_template=True) summary_message_cls=<class 'langchain.schema.messages.SystemMessage'> chat_memory=ChatMessageHistory(messages=[HumanMessage(content='What is USSA', additional_kwargs={}, example=False), AIMessage(content='USSA is a space agency in county Y. It is a government agency responsible for the exploration and development of space.', additional_kwargs={}, example=False), HumanMessage(content='Do you know about any space agency?', additional_kwargs={}, example=False), AIMessage(content='Yes, I mentioned earlier about the USSA, which is a government space agency responsible for space exploration and development in county Y.', additional_kwargs={}, example=False)]) output_key=None input_key='human_input' return_messages=False buffer='The human inquired about any space agency, and the AI reiterated its previous information about the USSA, a government space agency responsible for space exploration and development in county Y.' memory_key='chat_history'\n"
     ]
    }
   ],
   "source": [
    "print(qa_chain.memory) ## You can see the memory using this call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8542530f",
   "metadata": {},
   "source": [
    "## 3. Demonstrating user query based context summarization chain:\n",
    "Sometimes context can be large and don't fit in a prompt window. So, this chain summarizes context given user query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "539bc2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: \n",
      "USSA is a government space agency in county Y, responsible for the exploration and development of space.\n",
      "\n",
      "This llm extracts only relevant information from the context. \n"
     ]
    }
   ],
   "source": [
    "from chatbotSkills import user_query_based_context_summarization\n",
    "\n",
    "# Template \n",
    "prefix_template = \"\"\"\n",
    "    Write a concise summary of the context so that it includes the details related to the human query.\n",
    "    \"\"\"\n",
    "\n",
    "context_summary_chain = user_query_based_context_summarization(llm, prefix_template=prefix_template, to_debug=False)\n",
    "\n",
    "context = \"\"\"USSA is a space agency in county Y. It is a government agency responsible\n",
    "    for the exploration and development of space.\n",
    "    Zootopia is a 2016 American computer-animated buddy cop action comedy\n",
    "    film produced by Walt Disney Animation Studios.\n",
    "    \"\"\"\n",
    "\n",
    "#Question 1\n",
    "answer = context_summary_chain.run({\n",
    "    'context': context,\n",
    "    'human_input': \"What is USSA?\" \n",
    "})\n",
    "\n",
    "\n",
    "print(\"Question 1: \")\n",
    "print(answer)\n",
    "print()\n",
    "\n",
    "print (\"\"\"This llm extracts only relevant information from the context. \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34951a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human_prefix='Human' ai_prefix='AI' llm=AzureChatOpenAI(cache=None, verbose=True, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='48bb094bd3a24918b1be7d6de06d45ee', openai_api_base='https://journeyman-openai.openai.azure.com/', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None, deployment_name='gpt-4-32k', model_version='', openai_api_type='azure', openai_api_version='2023-05-15') prompt=PromptTemplate(input_variables=['summary', 'new_lines'], output_parser=None, partial_variables={}, template='\\n    Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\\n\\n    Current summary:\\n    {summary}\\n\\n    New lines of conversation:\\n    {new_lines}\\n\\n    New summary:', template_format='f-string', validate_template=True) summary_message_cls=<class 'langchain.schema.messages.SystemMessage'> chat_memory=ChatMessageHistory(messages=[HumanMessage(content='What is USSA', additional_kwargs={}, example=False), AIMessage(content='USSA is a space agency in county Y. It is a government agency responsible for the exploration and development of space.', additional_kwargs={}, example=False), HumanMessage(content='Do you know about any space agency?', additional_kwargs={}, example=False), AIMessage(content='Yes, I mentioned earlier about the USSA, which is a government space agency responsible for space exploration and development in county Y.', additional_kwargs={}, example=False)]) output_key=None input_key='human_input' return_messages=False buffer='The human inquired about any space agency, and the AI reiterated its previous information about the USSA, a government space agency responsible for space exploration and development in county Y.' memory_key='chat_history'\n"
     ]
    }
   ],
   "source": [
    "print(qa_chain.memory) ## You can see the memory using this call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2d397d",
   "metadata": {},
   "source": [
    "## 3. Demonstrating combine_docs:\n",
    "Sometimes contexts retrieved from the database can be large and doesn't fit in a prompt. So, this code will extract relevant information from the context given the user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6b9e2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_token_count:  134\n",
      "output_token_count:  33\n",
      "output:  USSA stands for United Space Exploration Administration, which is county Y's premier governmental space agency responsible for the exploration, investigation, and advancement of the cosmic frontier.\n",
      "\n",
      "This demonstrates that the combine_docs function reduces the tokens for the input\n"
     ]
    }
   ],
   "source": [
    "from chatbotSkills import combine_docs, count_tokens\n",
    "\n",
    "context_1 = \"\"\"\n",
    "    The United Space Exploration Administration (USSA) stands as county Y's premier governmental space agency, \n",
    "    entrusted with the monumental task of spearheading the exploration, investigation, and advancement of the cosmic frontier.\n",
    "    With a dedicated cadre of brilliant scientists, intrepid astronauts, \n",
    "    and cutting-edge technology, USSA pioneers a path to unlock the mysteries of the universe and \n",
    "    harness its potential for the betterment of humanity. \n",
    "    \"\"\"\n",
    "context_2 = \"\"\"\n",
    "    Through audacious missions and visionary initiatives, \n",
    "    USSA propels the nation to new heights, ensuring that the celestial realm \n",
    "    becomes a beacon of knowledge, opportunity, and inspiration for generations to come.\n",
    "    \"\"\"\n",
    "\n",
    "context_list = [context_1, context_2]\n",
    "\n",
    "input_token_count = count_tokens(context_1+context_2, engine)\n",
    "prefix_template = \"\"\"\n",
    "    Extract information from the context so that it includes the details related to the human query. \n",
    "    \"\"\"\n",
    "user_query = \"What does USSA tands for?\" \n",
    "max_input_tokens = 100 ## For demonstration, we are assuming that max token for input context should not exceed 100\n",
    "\n",
    "output = combine_docs(context_list, llm, to_debug=False, max_tokens=max_input_tokens, \n",
    "                      user_query=user_query, prefix_template=prefix_template)\n",
    "\n",
    "output_token_count = count_tokens(output, engine)\n",
    "print(\"input_token_count: \", input_token_count)\n",
    "print(\"output_token_count: \", output_token_count)\n",
    "\n",
    "print(\"output: \", output)\n",
    "print()\n",
    "print(\"\"\"This demonstrates that the combine_docs function reduces the tokens for the input\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a003e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nbdev",
   "language": "python",
   "name": "nbdev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
