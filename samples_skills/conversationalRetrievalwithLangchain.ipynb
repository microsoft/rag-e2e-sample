{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will demonstrate how to use LangChain to perform [conversational retrieval](https://python.langchain.com/docs/use_cases/question_answering/chat_vector_db).\n",
    "\n",
    "In a conversational question and answering scenario, users often pose follow-up questions related to the same topic, with the context being crucial to understand their queries. To address such cases effectively, we use the ConversationalRetrievalChain. Behind the scenes, this chain takes the user's question and converts it into a standalone query by considering the conversation history. Subsequently, it uses this standalone question to query the search service for relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install azure-identity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are utilizing an existing search index we already set up with Azure Cognitive Search. You can follow this [link](https://python.langchain.com/docs/integrations/vectorstores/azuresearch) to create your own search index. In our search index, the content vector field is named \"contentVector\", so we set it as an environment variable below.\n",
    "\n",
    "In this notebook we are focusing on the ConversationalRetrievalChain only for demonstration purposes. If you use your own search index you might need to modify your queries so that it is relevant to the information in your index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"AZURESEARCH_FIELDS_CONTENT_VECTOR\"] = \"contentVector\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import AzureOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../llm.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.azuresearch import AzureSearch\n",
    "\n",
    "\n",
    "model = os.getenv(\"OPENAI_DEPLOYMENT_EMBEDDING\")\n",
    "embeddings = OpenAIEmbeddings(deployment=model)\n",
    "index_name = \"testqa\"\n",
    "vector_store_address = os.getenv(\"COGSEARCH_ADDRESS\")\n",
    "vector_store_password = os.getenv(\"COGSEARCH_API_KEY\")\n",
    "vector_store = AzureSearch(\n",
    "    azure_search_endpoint=vector_store_address,\n",
    "    azure_search_key=vector_store_password,\n",
    "    index_name=index_name,\n",
    "    embedding_function=embeddings.embed_query,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "memory.output_key = \"answer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureOpenAI(deployment_name=os.getenv(\"OPENAI_DEPLOYMENT_COMPLETION\"), model_name=os.getenv(\"OPENAI_MODEL_COMPLETION\"),temperature=0)\n",
    "qa = ConversationalRetrievalChain.from_llm(llm, vector_store.as_retriever(search_kwargs={\"k\": 1}), memory=memory, return_generated_question=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What did the astronaut Edgar Mitchell call Earth?\"\n",
    "result = qa({\"question\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Edgar Mitchell called Earth a \"sparkling blue and white jewel.\"'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"When and how did NASA make its first observation about it from the space?\"\n",
    "result = qa({\"question\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' NASA first observed Earth from space with the launch of Explorer 1 in 1960.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, if we had query the search service using the user's question directly, it is not clear wht the user is reffering to by \"its\" and \"it\". As you can see below, the generated standalone question makes it more cleary, and enables for more efficient and robust queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' When and how did NASA first observe Earth from space?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"generated_question\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the user asks a question on a different topic, the generated question still reflects the user's question correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Why can't we see volcanic plumes with our eyes?\"\n",
    "result = qa({\"question\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volcanic plumes are not visible to the naked eye because they are typically invisible in the electromagnetic spectrum. However, satellites can use infrared to distinguish the plumes from ice and clouds.\n",
      " Why are volcanic plumes not visible to the naked eye?\n"
     ]
    }
   ],
   "source": [
    "print(result[\"answer\"])\n",
    "print(result[\"generated_question\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
