{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load configs for database, Azure OpenAI, and other resources as environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "\n",
    "# specify the name of the .env file name \n",
    "env_name = \"llm_pgvector.env\" # change to your own .env file name\n",
    "config = dotenv_values(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Flex Postgres (PG)  for retrievals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import pool\n",
    "from psycopg2 import Error\n",
    "\n",
    "host = config[\"HOST\"]\n",
    "dbname = config[\"DBNAME\"] \n",
    "user = config[\"USER\"] \n",
    "password = config[\"PASSWORD\"] \n",
    "sslmode = config[\"SSLMODE\"] \n",
    "\n",
    "# Build a connection string from the variables\n",
    "conn_string = \"host={0} user={1} dbname={2} password={3} sslmode={4}\".format(host, user, dbname, password, sslmode)\n",
    "\n",
    "postgreSQL_pool = psycopg2.pool.SimpleConnectionPool(1, 20,conn_string)\n",
    "if (postgreSQL_pool):\n",
    "    print(\"Connection pool created successfully\")\n",
    "\n",
    "# Use getconn() to get a connection from the connection pool\n",
    "connection = postgreSQL_pool.getconn()\n",
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare for question embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "openai.api_type = config[\"OPENAI_API_TYPE\"] \n",
    "openai.api_key = config['OPENAI_API_KEY']\n",
    "openai.api_base = config['OPENAI_API_BASE'] \n",
    "openai.api_version = config['OPENAI_API_VERSION']  \n",
    "\n",
    "\n",
    "def createEmbeddings(text):\n",
    "    response = openai.Embedding.create(input=text , engine=config[\"OPENAI_DEPLOYMENT_EMBEDDING\"])\n",
    "    embeddings = response['data'][0]['embedding']\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define llm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai.api_key\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://synapseml-openai.openai.azure.com/\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2022-12-01\"\n",
    "os.environ[\"OPENAI_DEPLOYMENT_NAME\"] = \"text-davinci-003\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import AzureOpenAI\n",
    "llm= AzureOpenAI(deployment_name=config[\"OPENAI_MODEL_COMPLETION\"], model_name=config[\"OPENAI_MODEL_EMBEDDING\"], temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders.base import BaseLoader\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "class TextFormatter(BaseLoader):\n",
    "    \"\"\"Load text files.\"\"\"\n",
    "\n",
    "    def __init__(self, text: str):\n",
    "        \"\"\"Initialize with file path.\"\"\"\n",
    "        self.text = text\n",
    "\n",
    "    def load(self) -> List[Document]:\n",
    "        \"\"\"Load from file path.\"\"\"\n",
    "        metadata = {\"source\": \"\"}\n",
    "        return [Document(page_content=self.text, metadata=metadata)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Navigate to the directory containing the CSV file (one level above the current directory)\n",
    "data_directory = os.path.abspath(os.path.join(current_directory, '..', 'ValidationSetOfQA'))\n",
    "\n",
    "# Construct the file path for your CSV file in the data_directory\n",
    "csv_file_path = os.path.join(data_directory, 'QnAValidationSet.csv')\n",
    "\n",
    "# Load the CSV file using pandas\n",
    "df = pd.read_csv(csv_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_id1_name = \"\"\n",
    "filter_id2_name = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [filter_id1_name, filter_id2_name, 'Question', 'Answer', 'ReferenceText', 'PageNumber']\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = df.copy()\n",
    "df_eval.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [item for pair in zip(list(df_eval['Question']), list(df_eval['Answer'])) for item in pair]\n",
    "keys = [str(i//2)+'a' if i%2==0 else str(i//2+1)+'q' for i in range(1,len(values)+2)]\n",
    "\n",
    "userQuestions = {keys[i]:values[i] for i in range(len(keys)-1)}\n",
    "filter_id1_vals = [item for item in list(df_eval[filter_id1_name]) for _ in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT based question answering with type checking\n",
    "from langchain import PromptTemplate\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_similarities(QA_results, verbose = False):\n",
    "    # compare cosine similarity between two vectors\n",
    "    cosine_similarities = []\n",
    "    for i in range(len(QA_results[0])):\n",
    "        if verbose:\n",
    "            print('calculating cosine similarity for: \\n', QA_results[0][i], '\\n', QA_results[1][i])\n",
    "            print(30*'-')\n",
    "        emd1 = createEmbeddings(QA_results[0][i])\n",
    "        emd2 = createEmbeddings(QA_results[1][i])\n",
    "        cosine_similarity_val = cosine_similarity(\n",
    "            np.array(emd1).reshape(1, -1), np.array(emd2).reshape(1, -1)\n",
    "        )[0][0]\n",
    "        cosine_similarities.append(np.round(cosine_similarity_val, 2))\n",
    "    return cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's also evaluate the reference page number\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_page_number(text):\n",
    "    # Regular expression pattern to find the PageNumber value\n",
    "    pattern = r'PageNumber:\\s+(\\d+)'\n",
    "\n",
    "    # Search for the pattern in the text\n",
    "    match = re.search(pattern, text)\n",
    "\n",
    "    # If a match is found, return the extracted PageNumber value, otherwise return 0\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_all_page_numbers(Agpt):\n",
    "    page_numbers = []\n",
    "    for answers in Agpt:\n",
    "        page_numbers.append(extract_page_number(answers))\n",
    "    print(page_numbers)\n",
    "    print(df_eval[\"PageNumber\"].tolist())\n",
    "    page_number_score = [1 if page_numbers[i] == df_eval[\"PageNumber\"].tolist()[i] else 0 for i in range(len(page_numbers))]\n",
    "    print(page_number_score)\n",
    "    return page_numbers, page_number_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgvector.psycopg2 import register_vector\n",
    "\n",
    "def retrieve_k_chunk(retrieve_k, questionEmbedding,filter_id1_val, similarity_method, verbose=False):\n",
    "    connection = psycopg2.connect(conn_string)\n",
    "# Create a cursor after the connection\n",
    "# Register 'pgvector' type for the 'embedding' column\n",
    "    register_vector(connection)\n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    #print(\"filter_id1_name:\", filter_id1_name)\n",
    "    select_docid_query = f\"SELECT DocId FROM {table_name1} WHERE {filter_id1_name} = '{filter_id1_val}'\"\n",
    "    cursor.execute(select_docid_query)\n",
    "    doc_id = cursor.fetchone()[0]\n",
    "    if verbose:\n",
    "        print('filter_id1_name:', filter_id1_name)\n",
    "        print('DocId:', doc_id)\n",
    "    \n",
    "    if similarity_method == 'NN':\n",
    "        sign = '<->'\n",
    "    elif similarity_method=='cosine':\n",
    "        sign = '<=>'\n",
    "    elif similarity_method=='inner':\n",
    "        sign = '<#>'\n",
    "    select_query = f\"SELECT Id FROM {table_name2} where DocId = '{doc_id}' ORDER BY embedding <-> %s LIMIT {retrieve_k}\"\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(select_query, (np.array(questionEmbedding),))\n",
    "    results = cursor.fetchall()\n",
    "    top_ids = []\n",
    "    for i in range(len(results)):\n",
    "        top_ids.append(int(results[i][0]))\n",
    "\n",
    "    if verbose:\n",
    "        print('top_ids:', top_ids)\n",
    "    # Rollback the current transaction\n",
    "    connection.rollback()\n",
    "\n",
    "    format_ids = ', '.join(['%s'] * len(top_ids))\n",
    "\n",
    "    sql = f\"SELECT CONCAT('PageNumber: ', PageNumber, ' ', 'LineNumber: ', LineNumber, ' ', 'Text: ', Chunk) AS concat FROM {table_name2} WHERE id IN ({format_ids})\"\n",
    "\n",
    "    # Execute the SELECT statement\n",
    "    try:\n",
    "        cursor.execute(sql, top_ids)    \n",
    "        top_rows = cursor.fetchall()\n",
    "    except (Exception, Error) as e:\n",
    "        print(f\"Error executing SELECT statement: {e}\")\n",
    "    finally:\n",
    "        pass\n",
    "        # cursor.close()\n",
    "    #print(\"top_rows\", top_rows)\n",
    "    # getting teh \n",
    "    sql_pages = f\"SELECT PageNumber FROM {table_name2} WHERE id IN ({format_ids})\"\n",
    "    # Execute the SELECT statement\n",
    "    try:\n",
    "        cursor.execute(sql_pages, top_ids)    \n",
    "        pages = cursor.fetchall()\n",
    "    except (Exception, Error) as e:\n",
    "        print(f\"Error executing SELECT statement: {e}\")\n",
    "    finally:\n",
    "        cursor.close()    \n",
    "    \n",
    "    retrieved_pages = [int(page[0]) for page in pages]\n",
    "    return top_rows, retrieved_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.metadata import packages_distributions\n",
    "\n",
    "\n",
    "def get_user_questions_answers(retrieve_k, filter_id1_vals,similarity_method, QUESTION_PROMPT, verbose=False):\n",
    "    \"\"\"\n",
    "    Collection of user questions with known answers.\n",
    "    \"\"\"\n",
    "    Q = []\n",
    "    A = []\n",
    "    Agpt = []\n",
    "    contexts = []\n",
    "    pages = []\n",
    "    i = 0\n",
    "    for key, value in userQuestions.items():\n",
    "        if \"q\" in key:\n",
    "            Q.append(value)\n",
    "            questionEmbedding = createEmbeddings(value)\n",
    "            if verbose:\n",
    "                print(\"question: \", value)\n",
    "            top_rows, retreived_pages = retrieve_k_chunk(retrieve_k, questionEmbedding,filter_id1_vals[i],similarity_method, verbose = verbose)\n",
    "            # create the context from the top_rows\n",
    "            context = \"\"\n",
    "            for row in top_rows:\n",
    "                context += row[0]\n",
    "                context += \"\\n\"\n",
    "            if verbose:\n",
    "                print('context: \\n', context)\n",
    "            loader = TextFormatter(context)\n",
    "            chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=QUESTION_PROMPT)\n",
    "            ans = chain({\"input_documents\": loader.load(), \"question\": value}, return_only_outputs=True)\n",
    "            Agpt.append(ans['output_text'])\n",
    "            contexts.append(context)\n",
    "            pages.append(retreived_pages)\n",
    "            if verbose:\n",
    "                print(ans['output_text'])\n",
    "            i+=2\n",
    "        else:\n",
    "            A.append(value)\n",
    "        \n",
    "        \n",
    "    return  Q, A, Agpt, contexts, pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_text(config_dict):\n",
    "    config_text = \"\"\n",
    "    for key, value in config_dict.items():\n",
    "        config_text += f\"{key}: {value}\\n\"\n",
    "    return config_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a filename with the timestamp as part of the extension\n",
    "  # You can choose any file name with an appropriate extension (.csv, .parquet, etc.)\n",
    "\n",
    "def run_experiment(ExperimentConfig, QUESTION_PROMPT, verbose=False):\n",
    "    config_text = dict_to_text(ExperimentConfig)\n",
    "    Q, A, Agpt, contexts, pages = get_user_questions_answers(retrieve_k = ExperimentConfig['retrieve_k'] ,filter_id1_vals=filter_id1_vals, similarity_method = ExperimentConfig['similarity_method'], QUESTION_PROMPT=QUESTION_PROMPT, verbose=verbose)\n",
    "    QAres = [A, Agpt, Q]\n",
    "    cos_sim_scores = get_cosine_similarities(QAres, verbose= True)\n",
    "    page_numbers, page_number_score = get_all_page_numbers(Agpt)\n",
    "    df_evaluation = pd.DataFrame({'Question': Q, 'Answer': A, 'Answer_gpt': Agpt, 'Score': cos_sim_scores, 'detected_page_number': page_numbers, 'actual_page_number': df_eval[\"PageNumber\"].tolist(), 'page_number_score': page_number_score, 'context': contexts, \\\n",
    "        'retrieved_pages': pages})\n",
    "    df_evaluation[\"correct_page_in_retrieved\"] = df_evaluation.apply(lambda row: row['actual_page_number'] in row['retrieved_pages'], axis=1)\n",
    "    df_evaluation[\"config\"] = config_text\n",
    "    df_evaluation[\"prompt\"] = QUESTION_PROMPT.template\n",
    "    current_timestamp = pd.Timestamp.now()\n",
    "    timestamp_str = current_timestamp.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    file_path ='..\\DATA\\evaluation'+ '_retrieve_'+ str(ExperimentConfig['retrieve_k']) + '_similarity_'+ ExperimentConfig['similarity_method']+'_date_' +timestamp_str + '_.csv'  \n",
    "    df_evaluation.to_csv(file_path, index=False)\n",
    "    return np.mean(cos_sim_scores), np.mean(page_number_score),  cos_sim_scores, page_number_score, df_evaluation, file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation using MLFLOW library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run multiple experiments using mlflow to compare results of various search methods, and number of top retrievals. Similarly, other parameter variation based experiments can be run and tracked using mlflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow, time\n",
    "mlflow.set_experiment(experiment_name=\"RAG_EXP\")\n",
    "    \n",
    "#import yaml\n",
    "#config = yaml.load(open(\"EvalConfig.yaml\", \"r\"), Loader=yaml.FullLoader)\n",
    "table_name1 = filter_id1_name\n",
    "table_name2 = 'ChunksEmbedding'\n",
    "ExperimentConfig1 = {'retrieve_k': 5, 'similarity_method': 'knn'} # similarity = ['cosine', 'NN', 'inner']\n",
    "ExperimentConfig2 = {'retrieve_k': 10, 'similarity_method': 'knn'} # similarity = ['cosine', 'NN', 'inner']\n",
    "ExperimentConfig3 = {'retrieve_k': 2, 'similarity_method': 'knn'} # similarity = ['cosine', 'NN', 'inner']\n",
    "ExperimentConfig4 = {'retrieve_k': 5, 'similarity_method': 'cosine'} # similarity = ['cosine', 'NN', 'inner']\n",
    "ExperimentConfig5 = {'retrieve_k': 5, 'similarity_method': 'inner'} # similarity = ['cosine', 'NN', 'inner']\n",
    "ExperimentConfig6 = {'retrieve_k': 5, 'similarity_method': 'knn'} # similarity = ['cosine', 'NN', 'inner']\n",
    "\n",
    "RUNConfigs = [ExperimentConfig1, ExperimentConfig2, ExperimentConfig3, ExperimentConfig4, ExperimentConfig5]\n",
    "question_prompt_template = \"\"\"Use the following portion of the context document to find relevant text and answer the question in details. Extract PageNumber and LineNumber and show it in the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "If the answer is not found, say that answer is not available in the documentation.\"\"\"\n",
    "QUESTION_PROMPT = PromptTemplate(\n",
    "    template=question_prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "for ExperimentConfig in RUNConfigs:\n",
    "    current_timestamp = pd.Timestamp.now()\n",
    "    timestamp_str = current_timestamp.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    mlflow_run_name = \"Retreive_k_\" + str(ExperimentConfig['retrieve_k']) + \"_similarity_\" + ExperimentConfig['similarity_method']+ \"_date_\" + timestamp_str\n",
    "    with mlflow.start_run(run_name=mlflow_run_name) as run:\n",
    "        mean_sim_score, mean_page_score,cos_sim_score, page_number_score, df_evaluation, df_path = run_experiment(ExperimentConfig, QUESTION_PROMPT, verbose = False)\n",
    "        print(\"mean_sim_score, mean_page_score:\", mean_sim_score, mean_page_score)\n",
    "        mlflow.log_metric(\"mean_sim_score\", mean_sim_score)\n",
    "        mlflow.log_metric(\"mean_page_score\", mean_page_score)\n",
    "        mlflow.log_param(\"cosine_similarity_score\", str(cos_sim_score))\n",
    "        mlflow.log_param(\"page_number_score\", str(page_number_score))\n",
    "        mlflow.log_param(\"Client code table\", table_name1)\n",
    "        mlflow.log_param(\"Chunk table\", table_name2)\n",
    "        mlflow.log_param(\"prompt\", QUESTION_PROMPT)  \n",
    "        mlflow.log_param(\"config\", \"\".join(ExperimentConfig))\n",
    "        mlflow.log_artifact(df_path)\n",
    "        print(\"config :\\n\", ExperimentConfig)\n",
    "        time.sleep(0.5)\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results will be saved in `\\mlruns` inside the `Notebooks` directory. To view visualization, run `mlflow ui` command from the Notebooks directory path level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flexenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
